{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annic\\OneDrive\\Bureau\\MainPhD\\code\\behavioural_exp_code\\exploit_results_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the needed packages \n",
    "# \n",
    "# 1/ the usual suspects\n",
    "import sys,os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax import vmap\n",
    "from jax.tree_util import tree_map\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# 2/ The Active Inference package \n",
    "import actynf\n",
    "from actynf.jaxtynf.jax_toolbox import _normalize,_jaxlog\n",
    "from actynf.jaxtynf.layer_trial import compute_step_posteriors\n",
    "from actynf.jaxtynf.layer_learn import learn_after_trial\n",
    "from actynf.jaxtynf.layer_options import get_learning_options,get_planning_options\n",
    "from actynf.jaxtynf.shape_tools import to_log_space,get_vectorized_novelty\n",
    "from actynf.jaxtynf.shape_tools import vectorize_weights\n",
    "\n",
    "# Weights for the active inference model : \n",
    "from simulate.hmm_weights import basic_latent_model\n",
    "\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array(-11.353539, dtype=float32), Array([-11.043939  ,  -0.30960083], dtype=float32))\n",
      "(Array(-11.353539, dtype=float32), Array([-11.043939  ,  -0.30960083], dtype=float32))\n",
      "[1.4 0.5] -7.659359\n",
      "[1.76 0.5 ] -6.1393585\n",
      "[2.084 0.5  ] -4.908159\n",
      "[2.3756 0.5   ] -3.9108868\n",
      "[2.63804 0.5    ] -3.1030965\n",
      "[2.874236 0.5     ] -2.4487863\n",
      "[3.0868125 0.5      ] -1.9187951\n",
      "[3.2781312 0.5      ] -1.489502\n",
      "[3.450318 0.5     ] -1.1417749\n",
      "[3.6052864 0.5      ] -0.860116\n",
      "[3.7447577 0.5      ] -0.63197196\n",
      "[3.870282 0.5     ] -0.4471755\n",
      "[3.9832537 0.5      ] -0.29749036\n",
      "[4.0849285 0.5      ] -0.17624533\n",
      "[4.1764355 0.5      ] -0.078036785\n",
      "[4.258792 0.5     ] 0.0015118122\n",
      "[4.332913 0.5     ] 0.06594646\n",
      "[4.3996215 0.5      ] 0.11813855\n",
      "[4.4596596 0.5      ] 0.16041398\n",
      "[4.513694 0.5     ] 0.1946572\n",
      "[4.5623245 0.5      ] 0.22239423\n",
      "[4.606092 0.5     ] 0.24486125\n",
      "[4.645483 0.5     ] 0.26305938\n",
      "[4.680935 0.5     ] 0.27779996\n",
      "[4.7128415 0.5      ] 0.28973985\n",
      "[4.741557 0.5     ] 0.29941112\n",
      "[4.767401 0.5     ] 0.30724478\n",
      "[4.790661 0.5     ] 0.31359005\n",
      "[4.811595 0.5     ] 0.3187297\n",
      "[4.8304353 0.5      ] 0.3228929\n",
      "[4.8473916 0.5      ] 0.32626504\n",
      "[4.8626523 0.5      ] 0.32899648\n",
      "[4.876387 0.5     ] 0.33120894\n",
      "[4.888748 0.5     ] 0.33300108\n",
      "[4.8998733 0.5      ] 0.33445263\n",
      "[4.909886 0.5     ] 0.33562845\n",
      "[4.918897 0.5     ] 0.33658087\n",
      "[4.9270077 0.5      ] 0.33735228\n",
      "[4.934307 0.5     ] 0.33797723\n",
      "[4.9408765 0.5      ] 0.33848333\n",
      "[4.946789 0.5     ] 0.33889335\n",
      "[4.95211 0.5    ] 0.3392254\n",
      "[4.9568987 0.5      ] 0.3394944\n",
      "[4.961209 0.5     ] 0.33971226\n",
      "[4.965088 0.5     ] 0.33988875\n",
      "[4.9685793 0.5      ] 0.34003174\n",
      "[4.971721 0.5     ] 0.3401475\n",
      "[4.9745493 0.5      ] 0.3402413\n",
      "[4.977094 0.5     ] 0.34031725\n",
      "[4.979385 0.5     ] 0.34037882\n",
      "[4.9814463 0.5      ] 0.34042865\n",
      "[4.9833016 0.5      ] 0.340469\n",
      "[4.9849715 0.5      ] 0.34050173\n",
      "[4.9864745 0.5      ] 0.3405282\n",
      "[4.9878273 0.5      ] 0.34054965\n",
      "[4.9890447 0.5      ] 0.34056705\n",
      "[4.99014 0.5    ] 0.34058112\n",
      "[4.991126 0.5     ] 0.3405925\n",
      "[4.9920135 0.5      ] 0.34060174\n",
      "[4.992812 0.5     ] 0.34060925\n",
      "[4.9935308 0.5      ] 0.34061533\n",
      "[4.994178 0.5     ] 0.34062022\n",
      "[4.99476 0.5    ] 0.3406242\n",
      "[4.995284 0.5     ] 0.34062743\n",
      "[4.9957557 0.5      ] 0.34063\n",
      "[4.99618 0.5    ] 0.34063214\n",
      "[4.996562 0.5     ] 0.34063387\n",
      "[4.996906 0.5     ] 0.34063524\n",
      "[4.9972153 0.5      ] 0.34063637\n",
      "[4.9974937 0.5      ] 0.34063727\n",
      "[4.9977446 0.5      ] 0.34063798\n",
      "[4.99797 0.5    ] 0.34063858\n",
      "[4.998173 0.5     ] 0.34063905\n",
      "[4.998356 0.5     ] 0.34063947\n",
      "[4.9985204 0.5      ] 0.34063977\n",
      "[4.998668 0.5     ] 0.34064007\n",
      "[4.998801 0.5     ] 0.34064025\n",
      "[4.998921 0.5     ] 0.34064043\n",
      "[4.9990287 0.5      ] 0.34064054\n",
      "[4.999126 0.5     ] 0.34064066\n",
      "[4.999213 0.5     ] 0.34064078\n",
      "[4.999292 0.5     ] 0.34064084\n",
      "[4.999363 0.5     ] 0.3406409\n",
      "[4.999427 0.5     ] 0.34064096\n",
      "[4.999484 0.5     ] 0.34064096\n",
      "[4.9995356 0.5      ] 0.34064102\n",
      "[4.999582 0.5     ] 0.34064102\n",
      "[4.999624 0.5     ] 0.34064108\n",
      "[4.9996614 0.5      ] 0.34064108\n",
      "[4.9996953 0.5      ] 0.34064108\n",
      "[4.999726 0.5     ] 0.34064108\n",
      "[4.999753 0.5     ] 0.34064108\n",
      "[4.999778 0.5     ] 0.34064108\n",
      "[4.9998 0.5   ] 0.34064114\n",
      "[4.99982 0.5    ] 0.34064114\n",
      "[4.9998384 0.5      ] 0.34064114\n",
      "[4.9998546 0.5      ] 0.34064114\n",
      "[4.9998693 0.5      ] 0.34064114\n",
      "[4.999882 0.5     ] 0.34064114\n",
      "[4.999894 0.5     ] 0.34064114\n",
      "[4.999894 0.5     ]\n",
      "[1.4 0.5] -7.659359\n",
      "[1.76 0.5 ] -6.1393585\n",
      "[2.084 0.5  ] -4.908159\n",
      "[2.3756 0.5   ] -3.9108868\n",
      "[2.63804 0.5    ] -3.1030965\n",
      "[2.874236 0.5     ] -2.4487863\n",
      "[3.0868125 0.5      ] -1.9187951\n",
      "[3.2781312 0.5      ] -1.489502\n",
      "[3.450318 0.5     ] -1.1417749\n",
      "[3.6052864 0.5      ] -0.860116\n",
      "[3.7447577 0.5      ] -0.63197196\n",
      "[3.870282 0.5     ] -0.4471755\n",
      "[3.9832537 0.5      ] -0.29749036\n",
      "[4.0849285 0.5      ] -0.17624533\n",
      "[4.1764355 0.5      ] -0.078036785\n",
      "[4.258792 0.5     ] 0.0015118122\n",
      "[4.332913 0.5     ] 0.06594646\n",
      "[4.3996215 0.5      ] 0.11813855\n",
      "[4.4596596 0.5      ] 0.16041398\n",
      "[4.513694 0.5     ] 0.1946572\n",
      "[4.5623245 0.5      ] 0.22239423\n",
      "[4.606092 0.5     ] 0.24486125\n",
      "[4.645483 0.5     ] 0.26305938\n",
      "[4.680935 0.5     ] 0.27779996\n",
      "[4.7128415 0.5      ] 0.28973985\n",
      "[4.741557 0.5     ] 0.29941112\n",
      "[4.767401 0.5     ] 0.30724478\n",
      "[4.790661 0.5     ] 0.31359005\n",
      "[4.811595 0.5     ] 0.3187297\n",
      "[4.8304353 0.5      ] 0.3228929\n",
      "[4.8473916 0.5      ] 0.32626504\n",
      "[4.8626523 0.5      ] 0.32899648\n",
      "[4.876387 0.5     ] 0.33120894\n",
      "[4.888748 0.5     ] 0.33300108\n",
      "[4.8998733 0.5      ] 0.33445263\n",
      "[4.909886 0.5     ] 0.33562845\n",
      "[4.918897 0.5     ] 0.33658087\n",
      "[4.9270077 0.5      ] 0.33735228\n",
      "[4.934307 0.5     ] 0.33797723\n",
      "[4.9408765 0.5      ] 0.33848333\n",
      "[4.946789 0.5     ] 0.33889335\n",
      "[4.95211 0.5    ] 0.3392254\n",
      "[4.9568987 0.5      ] 0.3394944\n",
      "[4.961209 0.5     ] 0.33971226\n",
      "[4.965088 0.5     ] 0.33988875\n",
      "[4.9685793 0.5      ] 0.34003174\n",
      "[4.971721 0.5     ] 0.3401475\n",
      "[4.9745493 0.5      ] 0.3402413\n",
      "[4.977094 0.5     ] 0.34031725\n",
      "[4.979385 0.5     ] 0.34037882\n",
      "[4.9814463 0.5      ] 0.34042865\n",
      "[4.9833016 0.5      ] 0.340469\n",
      "[4.9849715 0.5      ] 0.34050173\n",
      "[4.9864745 0.5      ] 0.3405282\n",
      "[4.9878273 0.5      ] 0.34054965\n",
      "[4.9890447 0.5      ] 0.34056705\n",
      "[4.99014 0.5    ] 0.34058112\n",
      "[4.991126 0.5     ] 0.3405925\n",
      "[4.9920135 0.5      ] 0.34060174\n",
      "[4.992812 0.5     ] 0.34060925\n",
      "[4.9935308 0.5      ] 0.34061533\n",
      "[4.994178 0.5     ] 0.34062022\n",
      "[4.99476 0.5    ] 0.3406242\n",
      "[4.995284 0.5     ] 0.34062743\n",
      "[4.9957557 0.5      ] 0.34063\n",
      "[4.99618 0.5    ] 0.34063214\n",
      "[4.996562 0.5     ] 0.34063387\n",
      "[4.996906 0.5     ] 0.34063524\n",
      "[4.9972153 0.5      ] 0.34063637\n",
      "[4.9974937 0.5      ] 0.34063727\n",
      "[4.9977446 0.5      ] 0.34063798\n",
      "[4.99797 0.5    ] 0.34063858\n",
      "[4.998173 0.5     ] 0.34063905\n",
      "[4.998356 0.5     ] 0.34063947\n",
      "[4.9985204 0.5      ] 0.34063977\n",
      "[4.998668 0.5     ] 0.34064007\n",
      "[4.998801 0.5     ] 0.34064025\n",
      "[4.998921 0.5     ] 0.34064043\n",
      "[4.9990287 0.5      ] 0.34064054\n",
      "[4.999126 0.5     ] 0.34064066\n",
      "[4.999213 0.5     ] 0.34064078\n",
      "[4.999292 0.5     ] 0.34064084\n",
      "[4.999363 0.5     ] 0.3406409\n",
      "[4.999427 0.5     ] 0.34064096\n",
      "[4.999484 0.5     ] 0.34064096\n",
      "[4.9995356 0.5      ] 0.34064102\n",
      "[4.999582 0.5     ] 0.34064102\n",
      "[4.999624 0.5     ] 0.34064108\n",
      "[4.9996614 0.5      ] 0.34064108\n",
      "[4.9996953 0.5      ] 0.34064108\n",
      "[4.999726 0.5     ] 0.34064108\n",
      "[4.999753 0.5     ] 0.34064108\n",
      "[4.999778 0.5     ] 0.34064108\n",
      "[4.9998 0.5   ] 0.34064114\n",
      "[4.99982 0.5    ] 0.34064114\n",
      "[4.9998384 0.5      ] 0.34064114\n",
      "[4.9998546 0.5      ] 0.34064114\n",
      "[4.9998693 0.5      ] 0.34064114\n",
      "[4.999882 0.5     ] 0.34064114\n",
      "[4.999894 0.5     ] 0.34064114\n",
      "[4.999894 0.5     ]\n"
     ]
    }
   ],
   "source": [
    "# Old version :\n",
    "def compute_log_probs(params_list, prior_dist_list):\n",
    "    # Check that the lists have the same length\n",
    "    assert len(params_list) == len(prior_dist_list), \"Lists must have the same length\"\n",
    "    \n",
    "    log_probs = []\n",
    "    \n",
    "    # Compute log_prob for each parameter and distribution pair\n",
    "    for param, dist in zip(params_list, prior_dist_list):\n",
    "        # Use the log_prob method from TensorFlow Probability distributions\n",
    "        log_prob = dist.log_prob(param)\n",
    "        log_probs.append(log_prob)\n",
    "    \n",
    "    # Convert the list of log probabilities into a JAX array (tensor)\n",
    "    return jnp.array(log_probs)\n",
    "\n",
    "\n",
    "prior_distributions = [tfd.Normal(5.0,1.0),tfd.Beta(10.0,10.0)]\n",
    "log_prior_param_func = lambda __params : jnp.sum(compute_log_probs(__params,prior_distributions))\n",
    "grad_func = jax.value_and_grad(log_prior_param_func)\n",
    "\n",
    "\n",
    "\n",
    "# Better version :\n",
    "def compute_log_prob(_it_param,_it_prior_dist):\n",
    "    _mapped = tree_map(lambda x,y : y.log_prob(x),_it_param,_it_prior_dist)\n",
    "    \n",
    "    if isinstance(_mapped,dict):\n",
    "        _mapped = list(_mapped.values())\n",
    "    \n",
    "    _params_lp = jnp.stack(_mapped)\n",
    "    return jnp.sum(_params_lp),_params_lp\n",
    "\n",
    "\n",
    "\n",
    "dict_ = {\n",
    "    \"a\" : 0.5,\n",
    "    \"b\" : 0.3\n",
    "}\n",
    "\n",
    "dict_lp = {\n",
    "    \"a\" : tfd.Normal(5.0,1.0),\n",
    "    \"b\" : tfd.Beta(10.0,10.0)\n",
    "}\n",
    "print(compute_log_prob(dict_,dict_lp))\n",
    "\n",
    "\n",
    "list_ =  [0.5,0.3]\n",
    "list_lp = [tfd.Normal(5.0,1.0),tfd.Beta(10.0,10.0)]\n",
    "print(compute_log_prob(list_,list_lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test grad for lists : \n",
    "list_lp = [tfd.Normal(5.0,1.0),tfd.Beta(10.0,10.0)]\n",
    "encoder = lambda x : list(x)\n",
    "graded_function = jax.value_and_grad(lambda x  : compute_log_prob(encoder(x),list_lp)[0])\n",
    "\n",
    "params = jnp.array([1.0,0.5])\n",
    "lr = 0.1\n",
    "for it in range(100):\n",
    "    log_prior_value,gradient = graded_function(params)\n",
    "    \n",
    "    params = jnp.clip(params + lr*jnp.array(gradient),1e-10)\n",
    "    \n",
    "    print(params,log_prior_value)\n",
    "print(params)\n",
    "\n",
    "\n",
    "# Test grad for dicts : \n",
    "dict_lp = {\n",
    "    \"a\" : tfd.Normal(5.0,1.0),\n",
    "    \"b\" : tfd.Beta(10.0,10.0)\n",
    "}\n",
    "encoder = lambda x : {\"a\":x[0],\"b\":x[1]}\n",
    "graded_function = jax.value_and_grad(lambda x  : compute_log_prob(encoder(x),dict_lp)[0])\n",
    "\n",
    "params = jnp.array([1.0,0.5])\n",
    "lr = 0.1\n",
    "for it in range(100):\n",
    "    log_prior_value,gradient = graded_function(params)\n",
    "    \n",
    "    params = jnp.clip(params + lr*jnp.array(gradient),1e-10)\n",
    "    \n",
    "    print(params,log_prior_value)\n",
    "print(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploit_results_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
