{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational modeling : RL algorithms in a virtual environment\n",
    "\n",
    "Question : under very low amounts of evidence, how do human sample a complex action space ? Can we infer some form of structure in this exploration ? Can Active Inference provide some answers regarding the mechanistic processes behind it ?\n",
    "\n",
    "\n",
    "\n",
    "First, we grab the data corresponding to the experiment we're interested in (here, experiment 002). We also remove the subjects that either had technical issues or had very suspicious results. *(we should provide a clear rule on subject exclusion here, maybe based on action variance across all dimensions or reaction times ?).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the task results for 90 subjects.\n",
      "89 subjects remaining after removing problematic subjects.\n"
     ]
    }
   ],
   "source": [
    "# Import the needed packages \n",
    "# \n",
    "# 1/ the usual suspects\n",
    "import sys,os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax import vmap\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# 2/ The Active Inference package \n",
    "import actynf\n",
    "from actynf.jaxtynf.jax_toolbox import _normalize,_jaxlog\n",
    "\n",
    "# 3/ Tools for : \n",
    "# a. Getting the raw data : \n",
    "from database_handling.database_extract import get_all_subject_data_from_internal_task_id\n",
    "from simulate.utils import remove_by_indices\n",
    "# b. Preprocessing the data :\n",
    "from analysis_tools.preprocess import OPTIONS_PREPROCESS_DEFAULT,get_preprocessed_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Except subjects for predictors :\n",
    "problematic_subjects_misc = [\"5c9cb670b472d0001295f377\"]\n",
    "        # This subject has read the instructions with one submission and ran\n",
    "        # the actual task with another, rendering statistics computed impossible to \n",
    "        # compare, this should be substracted from any statistical models based on\n",
    "        # instructional data, but can be kept for raw performance plots.\n",
    "# problematic_subjects_fraudulent =[\"6595ae358923ce48b037a0dc\"]\n",
    "        # This subject has very suspicious responses, including always putting both points in the same place\n",
    "        # and acting as quickly as possible, to be removed from all analysis ?\n",
    "\n",
    "\n",
    "# Import the data from the remote mongodb database & the imported prolific demographics :\n",
    "INTERNAL_TASK_ID = \"002\"\n",
    "TASK_RESULTS_ALL = get_all_subject_data_from_internal_task_id(INTERNAL_TASK_ID,None,\n",
    "                                        autosave=True,override_save=False,autoload=True)\n",
    "print(\"Loaded the task results for \" + str(len(TASK_RESULTS_ALL)) + \" subjects.\")\n",
    "\n",
    "# Each subject in task results has the following entries : \n",
    "# TASK_RESULT_FEATURES, TASK_RESULTS_EVENTS, TASK_RESULTS_DATA, TASK_RESULTS,RT_FB\n",
    "remove_these_subjects = []\n",
    "for index,entry in enumerate(TASK_RESULTS_ALL):\n",
    "    subj_dict,_,_,_ = entry\n",
    "    subj_name = subj_dict[\"subject_id\"]\n",
    "    if subj_name in problematic_subjects_misc:\n",
    "        remove_these_subjects.append(index)\n",
    "\n",
    "TASK_RESULTS = remove_by_indices(TASK_RESULTS_ALL,remove_these_subjects)\n",
    "print(str(len(TASK_RESULTS)) + \" subjects remaining after removing problematic subjects.\")\n",
    "\n",
    "\n",
    "LABELS = [entry[0] for entry in TASK_RESULTS]\n",
    "EVENTS = [entry[1] for entry in TASK_RESULTS]\n",
    "TRIAL_DATA = [entry[2] for entry in TASK_RESULTS]\n",
    "RT_FBS = [entry[3] for entry in TASK_RESULTS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the \"raw data\" is loaded, we can use the preprocessing pipeline described briefly [here](./computational_modeling_101_preprocessing.ipynb) to generate a dictionnary with the observations and actions ready for use in our models. \n",
    "\n",
    "We can generate several dictionnaries depending on how we want the data to be formatted. This is driven by the option dictionnary :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of the 9790.0 actions performed by our subjects, 8233.0 were 'valid' (84.1 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annic\\OneDrive\\Bureau\\MainPhD\\code\\behavioural_exp_code\\exploit_results_env\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\annic\\OneDrive\\Bureau\\MainPhD\\code\\behavioural_exp_code\\exploit_results_env\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of the 9790.0 feedback sequences potentially observed by our subjects, 8803 were 'valid' (89.9 %)\n",
      "dict_keys(['observations', 'actions'])\n"
     ]
    }
   ],
   "source": [
    "preprocessing_options = {\n",
    "    \"actions\":{\n",
    "        \"distance_bins\" : np.array([0.0,0.05,0.2,0.5,jnp.sqrt(2) + 1e-10]),\n",
    "        \"angle_N_bins\"  : 8,\n",
    "        \"position_N_bins_per_dim\" : 3\n",
    "    },\n",
    "    \"observations\":{\n",
    "        \"N_bins\" : 10,\n",
    "        \"observation_ends_at_point\" : 2\n",
    "    }\n",
    "}\n",
    "    # We can modify these at will\n",
    "\n",
    "data = get_preprocessed_data(TRIAL_DATA,RT_FBS,preprocessing_options,\n",
    "                            verbose=True,\n",
    "                            autosave=True,autoload=True,override_save=True,\n",
    "                            label=\"default\")\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annic\\OneDrive\\Bureau\\MainPhD\\code\\behavioural_exp_code\\exploit_results_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Now to the models ! \n",
    "# Let us grab a model of the training environment itself : \n",
    "\n",
    "# ENVIRONMENTAL CONSTANTS :\n",
    "GRID_SIZE = (7,7)\n",
    "START_COORD = [[5,1],[5,2],[4,1]]\n",
    "END_COORD = [0,6]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We define the environment as a state machine that outputs a feedback \n",
    "# every time an action is given to it : \n",
    "from actynf.jaxtynf.layer_process import initial_state_and_obs,process_update\n",
    "from actynf.jaxtynf.shape_tools import vectorize_weights\n",
    "\n",
    "class TrainingEnvironment :\n",
    "    def __init__(self,rng_key,a,b,c,d,e,u,T):\n",
    "        # Environment parameters\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c \n",
    "        self.d = d\n",
    "        self.e = e\n",
    "        self.u = u\n",
    "        \n",
    "        # Timing\n",
    "        self.Ntimesteps = T\n",
    "        self.t = 0\n",
    "        self.rng_key = rng_key\n",
    "        \n",
    "        # Inner state and last feedback\n",
    "        self.current_state = None\n",
    "        self.previous_observation = None\n",
    "        \n",
    "        self.update_vectorized_weights()\n",
    "    \n",
    "    def update_vectorized_weights(self):\n",
    "        self.vec_a,self.vec_b,self.vec_d = vectorize_weights(self.a,self.b,self.d,self.u)\n",
    "    \n",
    "    def reinit_trial(self):\n",
    "        self.t = 0\n",
    "        \n",
    "        self.rng_key,init_tmstp_key = jax.random.split(self.rng_key)\n",
    "        [s_d,s_idx,s_vect],[o_d,o_idx,o_vect] = initial_state_and_obs(init_tmstp_key,self.vec_a,self.vec_d)\n",
    "        \n",
    "        self.current_state = s_vect\n",
    "        self.previous_observation = o_vect\n",
    "        \n",
    "        return o_vect,0.0,False,self.t\n",
    "    \n",
    "    def step(self,action_chosen):\n",
    "        self.t = self.t + 1   # New timestep !\n",
    "        \n",
    "        if self.t == self.Ntimesteps: # This should not happen as we check below :\n",
    "            print(\"New trial ! The action has not been used here.\")\n",
    "            return self.reinit_trial()\n",
    "        \n",
    "        self.rng_key,timestep_rngkey = jax.random.split(self.rng_key)\n",
    "        [s_d,s_idx,s_vect],[o_d,o_idx,o_vect] = process_update(timestep_rngkey,self.current_state,self.vec_a,self.vec_b,action_chosen)\n",
    "        \n",
    "        \n",
    "        N_outcomes = o_vect[0].shape[0]\n",
    "        reward = (jnp.linspace(0,1,N_outcomes)*(o_vect[0] - self.previous_observation[0])).sum()\n",
    "        \n",
    "        \n",
    "        # The next timestep for the agent :\n",
    "        \n",
    "        self.current_state = s_vect\n",
    "        self.previous_observation = o_vect\n",
    "         \n",
    "        return o_vect,reward,(self.t == self.Ntimesteps-1),self.t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Trial 0----\n",
      "0 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "1 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.22222222\n",
      "2 [Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.22222222\n",
      "3 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.44444445\n",
      "4 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "5 [Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.44444445\n",
      "6 [Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "7 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.44444445\n",
      "8 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "9 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "----\n",
      "----Trial 1----\n",
      "0 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "1 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.11111112\n",
      "2 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.11111112\n",
      "3 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.11111112\n",
      "4 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "5 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.111111104\n",
      "6 [Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)] 0.111111134\n",
      "7 [Array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)] 0.111111104\n",
      "8 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] -0.22222224\n",
      "9 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.111111104\n",
      "----\n",
      "----Trial 2----\n",
      "0 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "1 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "2 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.11111112\n",
      "3 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "4 [Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)] 0.33333337\n",
      "5 [Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "6 [Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.44444448\n",
      "7 [Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.11111111\n",
      "8 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.33333334\n",
      "9 [Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)] 0.22222224\n",
      "----\n",
      "----Trial 3----\n",
      "0 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "1 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "2 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.11111112\n",
      "3 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.22222222\n",
      "4 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "5 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.111111104\n",
      "6 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.11111112\n",
      "7 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.11111112\n",
      "8 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.111111104\n",
      "9 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.22222222\n",
      "----\n",
      "----Trial 4----\n",
      "0 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "1 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.22222222\n",
      "2 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "3 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "4 [Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)] 0.111111134\n",
      "5 [Array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)] 0.111111104\n",
      "6 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.33333334\n",
      "7 [Array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)] 0.33333334\n",
      "8 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] -0.22222224\n",
      "9 [Array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32)] 0.44444445\n",
      "----\n",
      "----Trial 5----\n",
      "0 [Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "1 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.33333334\n",
      "2 [Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)] 0.111111134\n",
      "3 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] -0.111111134\n",
      "4 [Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)] 0.111111134\n",
      "5 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] -0.111111134\n",
      "6 [Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.33333334\n",
      "7 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.33333334\n",
      "8 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "9 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "----\n",
      "----Trial 6----\n",
      "0 [Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "1 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.11111111\n",
      "2 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.22222222\n",
      "3 [Array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32)] 0.44444445\n",
      "4 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.5555556\n",
      "5 [Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.33333334\n",
      "6 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.22222222\n",
      "7 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "8 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.22222222\n",
      "9 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "----\n",
      "----Trial 7----\n",
      "0 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "1 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.11111112\n",
      "2 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.22222222\n",
      "3 [Array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)] 0.22222224\n",
      "4 [Array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)] 0.0\n",
      "5 [Array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)] 0.0\n",
      "6 [Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)] -0.111111104\n",
      "7 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.22222224\n",
      "8 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.111111104\n",
      "9 [Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)] 0.111111134\n",
      "----\n",
      "----Trial 8----\n",
      "0 [Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "1 [Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "2 [Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.11111111\n",
      "3 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.22222224\n",
      "4 [Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.22222224\n",
      "5 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.11111111\n",
      "6 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "7 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.11111112\n",
      "8 [Array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)] 0.33333334\n",
      "9 [Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.33333334\n",
      "----\n",
      "----Trial 9----\n",
      "0 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "1 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.22222222\n",
      "2 [Array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)] 0.44444448\n",
      "3 [Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] -0.5555556\n",
      "4 [Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)] 0.11111111\n",
      "5 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.22222222\n",
      "6 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "7 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "8 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "9 [Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)] 0.0\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The environment is statically defined by its HMM matrices : \n",
    "from simulate.hmm_weights import behavioural_process\n",
    "\n",
    "T = 10\n",
    "(a,b,c,d,e,u),fb_vals = behavioural_process(GRID_SIZE,START_COORD,END_COORD,preprocessing_options[\"observations\"][\"N_bins\"],0.15)\n",
    "\n",
    "rngkey = jax.random.PRNGKey(np.random.randint(0,10))\n",
    "env = TrainingEnvironment(rngkey,a,b,c,d,e,u,T)\n",
    "    \n",
    "for k in range(10):\n",
    "    print(\"----Trial {}----\".format(k))\n",
    "    o,r,end_trial,t = env.reinit_trial()\n",
    "    print(t,o,r)\n",
    "    while not(end_trial) :\n",
    "        o,r,end_trial,t = env.step(jax.nn.one_hot(1,9))\n",
    "        print(t,o,r)\n",
    "    print(\"----\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put all the models we are going to fit in an equal footing, we will assume a discretized action space along the 3 dimensions previously mentionned for all of them. To study the effect of not making this crucial hypothesis, we refer the reader to the Reinforcment Learning part of this study. \n",
    "\n",
    "Reward definition : we will assume that agents perceive changes in the level of the gauge as a reward signal.\n",
    "\n",
    "Here, we will study the following models : \n",
    "### Blind models - the action does not depend on the stimuli at all\n",
    "1. A random action model that selects actions randomly and does not learn\n",
    "2. A choice kernel model that repeats actions it already performed :  $CK_{t+1}(u) = CK_{t}(u) + \\alpha_c (a_t - CK_{t}(u))$ where $a_t$ is 1 if $u$ was selcted and 0.0 otherwise. $P_t(u) = \\sigma(\\beta_c CK_{t})$\n",
    "### Stateless models - the action does not depend on the level of the gauge but only on the reward\n",
    "3. A Rescorla-Wagner model - stateless that attempts to learn the value of a specific action by updating the perceived reward attached to it : $Q_{t+1}(u) = Q_{t}(u) + \\alpha (r_t - Q_{t}(u))$  (Wilson RC, Collins AG. Ten simple rules for the computational modeling of behavioral data. Elife. 2019 Nov 26;8:e49547. doi: 10.7554/eLife.49547. PMID: 31769410; PMCID: PMC6879303.). Actions are selected through a tempered softmax following : $P_t(u) = \\sigma(\\beta Q_{t})$. The free parameters for this model are $\\alpha$ and $\\beta$.\n",
    "4. A mix of the previous 2 models : $P_t(u) = \\sigma(\\beta Q_{t} + \\beta_c CK_{t})$\n",
    "### Stateful models - the action depends on the level of the gauge as well as the reward\n",
    "5. A tabular Q-learning agent : $Q_{t+1}(u,s) = Q_{t}(u,s) + \\alpha (r_t - Q_{t}(u,s))$ with different learning rates for positive and negative feedbacks $\\{\\alpha_-,\\alpha_+\\}$ with a choice temperature $\\beta$.\n",
    "6. A vesion of this agent that takes into account the choice Kernel described previously\n",
    "### Bayesian Active Inference models - The reward is not taken into account. The level of the gauge is interpreted as an indicator regarding a true (hidden) state of the system\n",
    "7. A single dimension model (\"How far am I from my goal ?\") with various subcategories of models : \n",
    "    - A naive q learning like model  with low amounts of states\n",
    "    - A more advanced model that features the ability to generalize between states\n",
    "    - An even more advanced model that also features the ability to generalize between actions\n",
    "8. A bidimensionnal model (\"Where am I in the grid ? Where is my goal ?\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Because we have a much more complex intuition behind what is happening but with a pretty hard-to-derive likelihood function, we should look at Simulation Based Inference (see https://elifesciences.org/articles/56261#s4,https://astroautomata.com/blog/simulation-based-inference/,Artificial neural networks for model identification and parameter estimation in computational cognitive models, Milena Rmus ,Ti-Fen Pan,Liyu Xia,Anne G. E. Collins, Published: May 15, 2024; https://doi.org/10.1371/journal.pcbi.1012119 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def random_agent(hyperparameters,constants):\n",
    "    # a,b,c = hyperparameters\n",
    "    num_actions, = constants\n",
    "    \n",
    "    \n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        return None # A function of the hyperparameters\n",
    "    \n",
    "    def initial_state(params):\n",
    "        # Initial agent state (beginning of each trial)\n",
    "        return None\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        gauge_level,reward,trial_over,t = observation\n",
    "        \n",
    "        # OPTIONAL : Update states based on previous states, observations and parameters\n",
    "        new_state = state\n",
    "        \n",
    "        # Compute action distribution using observation, states and parameters\n",
    "        action_distribution,_ = _normalize(jnp.ones((num_actions,)))\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0])\n",
    "        \n",
    "        return new_state,(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        rewards,observations,states,actions = trial_history\n",
    "        \n",
    "        # Trial history is a list of trial rewards, observations and states, we may want to make them jnp arrays :\n",
    "        # reward_array = jnp.stack(rewards)\n",
    "        # observation_array = jnp.stack(observations)\n",
    "        # states_array = jnp.stack(states)\n",
    "        # action_array = jnp.stack(actions)\n",
    "        \n",
    "        # OPTIONAL :  Update parameters based on states, observations and actions history\n",
    "        return None\n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    return initial_params,initial_state,actor_step,update_params\n",
    "\n",
    "\n",
    "def run_loop(environment,agent_functions,seed,Ntrials):\n",
    "    rng_key = jr.PRNGKey(seed)\n",
    "    init_params,init_state,agent_step,agent_learn = agent_functions\n",
    "    \n",
    "    params = init_params()\n",
    "    for trial in range(Ntrials):\n",
    "        print(\"Trial {}\".format(trial))\n",
    "        \n",
    "        o,r,end_trial,t = environment.reinit_trial()\n",
    "        \n",
    "        state = init_state(params)\n",
    "\n",
    "        rewards,observations,states,actions = [r],[o],[],[]\n",
    "\n",
    "        while not end_trial:\n",
    "            \n",
    "            choice_rng_key, rng_key = jr.split(rng_key)\n",
    "            state,(u_d,u_idx,u_vec) = agent_step((o,r,end_trial,t),state,params,choice_rng_key)\n",
    "        \n",
    "            o,r,end_trial,t = environment.step(u_vec)\n",
    "            \n",
    "            # The history of experiences for this trial :\n",
    "            states.append(state)\n",
    "            actions.append(u_vec)\n",
    "            \n",
    "            # And the observations and rewards for the next trial : \n",
    "            rewards.append(r)\n",
    "            observations.append(o)\n",
    "        \n",
    "        # Last state update :\n",
    "        choice_rng_key, rng_key = jr.split(rng_key)\n",
    "        state,_ = agent_step((o,r,end_trial,t),state,params,choice_rng_key)\n",
    "        states.append(state)\n",
    "        \n",
    "        # Parameter update (once every trial)\n",
    "        history = (rewards,observations,states,actions)\n",
    "        params = agent_learn(history,params)\n",
    "    return params\n",
    "            \n",
    "# In : an agent based on some hyperparameters : \n",
    "SEED = 100\n",
    "NTRIALS = 10\n",
    "random_agent_hyperparameters = None\n",
    "random_agent_constants = (9,)\n",
    "\n",
    "p = run_loop(env,random_agent(random_agent_hyperparameters,random_agent_constants),SEED,NTRIALS)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([4.8828218e-04, 1.1434174e-01, 1.9074696e-06, 7.8125000e-03,\n",
       "       1.2500095e-01, 7.5003064e-01, 4.0531158e-06, 1.9531439e-03,\n",
       "       3.6674924e-04], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def choice_kernel_agent(hyperparameters,constants):\n",
    "    alpha,beta = hyperparameters\n",
    "    num_actions, = constants\n",
    "\n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        # Parameters is the initial choice kernel :\n",
    "        CK_initial = jnp.zeros((num_actions,))\n",
    "        \n",
    "        return CK_initial # A function of the hyperparameters\n",
    "    \n",
    "    def initial_state(params):\n",
    "        # Initial agent state (beginning of each trial)\n",
    "        # The initial state is the CK table and an initial action (easier integration with rw+ck model)\n",
    "        return params,jnp.zeros((num_actions,))\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        gauge_level,reward,trial_over,t = observation\n",
    "        \n",
    "        # The state of the agent stores the choice kernel and the last action performed : \n",
    "        ck,last_action = state\n",
    "        \n",
    "        # Update the choice kernel :\n",
    "        was_a_last_action = jnp.sum(last_action)  # No update if there was no last action\n",
    "        new_ck = ck + alpha*(last_action - ck)*was_a_last_action\n",
    "        \n",
    "        action_distribution = jax.nn.softmax(beta*ck)\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0])        \n",
    "        \n",
    "        return (new_ck,vect_action_selected),(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        rewards,observations,states,actions = trial_history\n",
    "        \n",
    "        # The params for the next step is the last choice kernel of the trial :\n",
    "        # (the update already occured during the actor step !)\n",
    "        ck_last,previous_action_last = states[-1]\n",
    "        \n",
    "        return ck_last\n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    return initial_params,initial_state,actor_step,update_params\n",
    "\n",
    "\n",
    "ck_agent_hyperparameters = (0.5,1.0)\n",
    "ck_agent_constants = (9,)\n",
    "\n",
    "run_loop(env,choice_kernel_agent(ck_agent_hyperparameters,ck_agent_constants),SEED,NTRIALS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n",
      "[-0.02777778  0.11714002  0.25986737 -0.04166667 -0.0718316  -0.22336155\n",
      " -0.11523438  0.00271267  0.01025391]\n"
     ]
    }
   ],
   "source": [
    "def rescorla_wagner_agent(hyperparameters,constants):\n",
    "    alpha,beta = hyperparameters\n",
    "    num_actions, = constants\n",
    "\n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        # Parameters is the initial perceived reward :\n",
    "        q_initial = jnp.zeros((num_actions,))\n",
    "        \n",
    "        return q_initial # A function of the hyperparameters\n",
    "    \n",
    "    def initial_state(params):\n",
    "        # Initial agent state (beginning of each trial)\n",
    "        \n",
    "        # The initial state is the q_table, as well as an initial action selected (None)\n",
    "        return params,jnp.zeros((num_actions,))\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        gauge_level,reward,trial_over,t = observation\n",
    "        \n",
    "        q_t,previous_action = state\n",
    "        \n",
    "        # Update the table now that we have the new reward !\n",
    "        q_tplus = q_t + alpha*(reward-q_t)*previous_action\n",
    "        \n",
    "        action_distribution = jax.nn.softmax(beta*q_tplus)\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0])       \n",
    "        \n",
    "        return (q_tplus,vect_action_selected),(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        rewards,observations,states,actions = trial_history\n",
    "        \n",
    "        q_t_last,previous_action_last = states[-1]\n",
    "        \n",
    "        # The params for the next step is the last choice kernel of the trial :\n",
    "        # (the update already occured during the actor step !)\n",
    "        return q_t_last\n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    return initial_params,initial_state,actor_step,update_params\n",
    "\n",
    "\n",
    "rw_agent_hyperparameters = (0.5,1.0)\n",
    "rw_agent_constants = (9,)\n",
    "\n",
    "p = run_loop(env,rescorla_wagner_agent(rw_agent_hyperparameters,rw_agent_constants),SEED,NTRIALS)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n",
      "(Array([ 0.01332222,  0.04856306,  0.14029677,  0.07008401,  0.00227871,\n",
      "       -0.10464396, -0.08860292, -0.00578778, -0.02115816], dtype=float32), Array([0.04070239, 0.2859392 , 0.09573993, 0.05887281, 0.09927601,\n",
      "       0.23547989, 0.02822838, 0.06499574, 0.09068947], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "def rw_ck_agent(hyperparameters,constants):\n",
    "    alpha,beta,alpha_ck,beta_ck = hyperparameters\n",
    "    num_actions, = constants\n",
    "\n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        # Parameters are the initial perceived reward :\n",
    "        q_initial = jnp.zeros((num_actions,))\n",
    "        # and the initial choice kernel :\n",
    "        ck_initial = jnp.zeros((num_actions,))\n",
    "        \n",
    "        return q_initial,ck_initial \n",
    "    \n",
    "    def initial_state(params):\n",
    "        # Initial agent state (beginning of each trial)\n",
    "        q,ck = params\n",
    "        # The initial state is the q_table, as well as an initial action selected (None)\n",
    "        return q,ck,jnp.zeros((num_actions,))\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        gauge_level,reward,trial_over,t = observation\n",
    "        \n",
    "        q_t,ck,previous_action = state\n",
    "        \n",
    "        # Update the table now that we have the new reward !\n",
    "        q_tplus = q_t + alpha*(reward-q_t)*previous_action\n",
    "        \n",
    "        # Update the choice kernel :\n",
    "        was_a_last_action = jnp.sum(previous_action)  # No update if there was no last action\n",
    "        new_ck = ck + alpha*(previous_action - ck)*was_a_last_action\n",
    "        \n",
    "        \n",
    "        action_distribution = jax.nn.softmax(beta*q_tplus + beta_ck*ck)\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0]) \n",
    "        \n",
    "        return (q_tplus,new_ck,vect_action_selected),(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        rewards,observations,states,actions = trial_history\n",
    "        \n",
    "        q_t_last,ck_last,previous_action_last = states[-1]\n",
    "        \n",
    "        # The params for the next step is the last choice kernel of the trial :\n",
    "        # (the update already occured during the actor step !)\n",
    "        return q_t_last,ck_last\n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    return initial_params,initial_state,actor_step,update_params\n",
    "\n",
    "\n",
    "rw_ck_agent_hyperparameters = (0.1,0.5,0.5,0.1)\n",
    "rw_ck_agent_constants = (9,)\n",
    "\n",
    "p = run_loop(env,rw_ck_agent(rw_ck_agent_hyperparameters,rw_ck_agent_constants),SEED,NTRIALS)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n",
      "(Array([[ 0.2777778 ,  0.        , -0.2111111 ,  0.        , -0.23333333,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.2244089 ,  0.05555556, -0.06311111,  0.        ,\n",
      "        -0.14      , -0.3888889 , -0.31111112,  0.        ,  0.        ],\n",
      "       [-0.02222223,  0.04222222, -0.07777778, -0.03111111,  0.        ,\n",
      "        -0.0777778 ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.03333333,  0.05555556,  0.028     , -0.23333333,\n",
      "         0.        ,  0.        , -0.31111112,  0.        ,  0.        ],\n",
      "       [ 0.11111111,  0.        , -0.1111111 ,  0.21413335,  0.        ,\n",
      "         0.        , -0.31111112,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.0274311 ,  0.18222222, -0.15555556,\n",
      "        -0.23333336, -0.31111112,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.05555556,  0.        , -0.01111111, -0.15555556, -0.15555556,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.16666667,  0.07409778, -0.07777778,  0.        ,  0.        ,\n",
      "        -0.23333336,  0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [-0.01111111, -0.08666666,  0.04444445,  0.        , -0.03733334,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]],      dtype=float32), Array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def q_learning_agent(hyperparameters,constants):\n",
    "    alpha_plus,alpha_minus,beta,alpha_ck,beta_ck = hyperparameters\n",
    "    num_actions,num_states = constants\n",
    "\n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        # Parameters are the initial q-table. As opposed to a RW agent, the mappings now depend on the states \n",
    "        # This usually allows for better responsiveness to the environment, but in this situation it may make the training\n",
    "        # harder !\n",
    "        q_initial = jnp.zeros((num_actions,num_states))\n",
    "        # and the initial choice kernel :\n",
    "        ck_initial = jnp.zeros((num_actions,))\n",
    "        \n",
    "        return q_initial,ck_initial \n",
    "    \n",
    "    def initial_state(params):\n",
    "        # Initial agent state (beginning of each trial)\n",
    "        q,ck = params\n",
    "        # The initial state is the q_table, as well as an initial action selected (None) and the last gauge level (None)\n",
    "        return q,ck,jnp.zeros((num_actions,)),jnp.zeros((num_states,))\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        observations,reward,trial_over,t = observation\n",
    "        \n",
    "        gauge_level = observations[0]\n",
    "        \n",
    "        q_t,ck,previous_action,previous_gauge_level = state\n",
    "        \n",
    "        # Update the table now that we have the new reward !\n",
    "        # This is \"where\" the reward was observed in the table : \n",
    "        previous_action_state = jnp.einsum(\"i,j->ij\",previous_action,previous_gauge_level)\n",
    "        \n",
    "        positive_reward = jnp.clip(reward,min=0.0)\n",
    "        negative_reward = jnp.clip(reward,max=0.0)\n",
    "        \n",
    "        positive_reward_prediction_error = positive_reward - q_t\n",
    "        negative_reward_prediction_error = negative_reward - q_t\n",
    "        \n",
    "        q_tplus = q_t + (alpha_plus*positive_reward_prediction_error + alpha_minus*negative_reward_prediction_error)*previous_action_state\n",
    "        \n",
    "        # Update the choice kernel :\n",
    "        was_a_last_action = jnp.sum(previous_action)  # No update if there was no last action\n",
    "        new_ck = ck + alpha_ck*(previous_action - ck)*was_a_last_action\n",
    "        \n",
    "\n",
    "\n",
    "        # Action selection :\n",
    "        q_table_at_this_state = jnp.einsum(\"ij,j->i\",q_tplus,gauge_level)\n",
    "        \n",
    "        action_distribution = jax.nn.softmax(beta*q_table_at_this_state + beta_ck*new_ck)\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0])  \n",
    "        \n",
    "        return (q_tplus,new_ck,vect_action_selected,gauge_level),(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        rewards,observations,states,actions = trial_history\n",
    "        \n",
    "        q_t_last,ck_last,previous_action_last,previous_stimuli_last = states[-1]\n",
    "        \n",
    "        # The params for the next step is the last choice kernel of the trial :\n",
    "        # (the update already occured during the actor step !)\n",
    "        return q_t_last,ck_last\n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    return initial_params,initial_state,actor_step,update_params\n",
    "\n",
    "\n",
    "ql_ck_agent_hyperparameters = (0.5,0.7,1.0,0.0,0.0)\n",
    "ql_ck_agent_constants = (9,10)\n",
    "\n",
    "p = run_loop(env,q_learning_agent(ql_ck_agent_hyperparameters,ql_ck_agent_constants),SEED,NTRIALS)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "(10,)\n",
      "(10, 5)\n",
      "(10, 5)\n",
      "Trial 1\n",
      "(10,)\n",
      "(10, 5)\n",
      "(10, 5)\n",
      "Trial 2\n",
      "(10,)\n",
      "(10, 5)\n",
      "(10, 5)\n",
      "Trial 3\n",
      "(10,)\n",
      "(10, 5)\n",
      "(10, 5)\n",
      "Trial 4\n",
      "(10,)\n",
      "(10, 5)\n",
      "(10, 5)\n",
      "Trial 5\n",
      "(10,)\n",
      "(10, 5)\n",
      "(10, 5)\n",
      "Trial 6\n",
      "(10,)\n",
      "(10, 5)\n",
      "(10, 5)\n",
      "Trial 7\n",
      "(10,)\n",
      "(10, 5)\n",
      "(10, 5)\n",
      "Trial 8\n",
      "(10,)\n",
      "(10, 5)\n",
      "(10, 5)\n",
      "Trial 9\n",
      "(10,)\n",
      "(10, 5)\n",
      "(10, 5)\n",
      "([Array([[51.317047 , 12.445027 ,  1.2956123,  1.0005195,  1.0000001],\n",
      "       [33.263165 , 23.292961 ,  2.7491717,  1.009338 ,  1.0000056],\n",
      "       [14.263904 , 28.841404 ,  7.6361294,  1.1076207,  1.0001982],\n",
      "       [ 4.496297 , 23.293953 , 17.142466 ,  1.795261 ,  1.0044489],\n",
      "       [ 1.5909047, 12.446046 , 26.176617 ,  4.7678733,  1.0640323],\n",
      "       [ 1.0640323,  4.767873 , 26.176615 , 12.446048 ,  1.5909051],\n",
      "       [ 1.0044489,  1.7952611, 17.142466 , 23.293951 ,  4.496297 ],\n",
      "       [ 1.0001982,  1.1076207,  7.6361294, 28.8414   , 14.263905 ],\n",
      "       [ 1.0000056,  1.009338 ,  2.7491734, 23.292965 , 33.263157 ],\n",
      "       [ 1.0000001,  1.0005195,  1.2956123, 12.445029 , 51.317055 ]],      dtype=float32)], [Array([[[2.7291772, 4.2061644, 3.7055142, 2.8471732, 2.8603098,\n",
      "         3.5946646, 2.8656385, 3.143552 , 3.9485307],\n",
      "        [1.579748 , 2.0337079, 1.6026336, 1.169717 , 1.8671602,\n",
      "         2.2155623, 1.584792 , 2.2173414, 2.0998015],\n",
      "        [1.1514773, 1.1519866, 1.1219418, 1.0301282, 1.381723 ,\n",
      "         1.3801522, 1.2883446, 1.4338273, 1.3446121],\n",
      "        [1.0247917, 1.0246316, 1.0208081, 1.0085815, 1.0982224,\n",
      "         1.0807873, 1.1387777, 1.0457131, 1.0427959],\n",
      "        [1.0027877, 1.0059218, 1.0040566, 1.0010021, 1.0120908,\n",
      "         1.0078646, 1.0202205, 1.0138859, 1.007665 ]],\n",
      "\n",
      "       [[1.5306942, 2.833386 , 2.4506993, 2.0782597, 1.5404537,\n",
      "         1.9125915, 1.5315704, 1.3774502, 1.8127363],\n",
      "        [2.7291772, 4.2061644, 3.7055142, 2.8471732, 2.8603098,\n",
      "         3.5946646, 2.8656385, 3.143552 , 3.9485307],\n",
      "        [1.579748 , 2.0337079, 1.6026336, 1.169717 , 1.8671602,\n",
      "         2.2155626, 1.5847921, 2.2173414, 2.0998015],\n",
      "        [1.1514773, 1.1519866, 1.1219418, 1.0301282, 1.381723 ,\n",
      "         1.3801522, 1.2883446, 1.4338273, 1.3446121],\n",
      "        [1.0247917, 1.0246316, 1.0208081, 1.0085815, 1.0982224,\n",
      "         1.0807873, 1.1387777, 1.0457131, 1.0427959]],\n",
      "\n",
      "       [[1.1688275, 1.5119117, 1.6223142, 1.391641 , 1.1503485,\n",
      "         1.1939363, 1.1233662, 1.0565788, 1.1294485],\n",
      "        [1.5306942, 2.833386 , 2.4506993, 2.0782595, 1.5404537,\n",
      "         1.9125915, 1.5315703, 1.3774502, 1.8127363],\n",
      "        [2.7291772, 4.2061644, 3.7055142, 2.8471732, 2.8603098,\n",
      "         3.5946646, 2.8656385, 3.143552 , 3.9485307],\n",
      "        [1.579748 , 2.0337079, 1.6026336, 1.169717 , 1.8671602,\n",
      "         2.2155626, 1.5847921, 2.2173414, 2.0998015],\n",
      "        [1.1514773, 1.1519866, 1.1219418, 1.0301282, 1.381723 ,\n",
      "         1.3801522, 1.2883446, 1.4338273, 1.3446121]],\n",
      "\n",
      "       [[1.0141711, 1.1035784, 1.222798 , 1.041278 , 1.029009 ,\n",
      "         1.024745 , 1.0155784, 1.0179418, 1.0227755],\n",
      "        [1.1688275, 1.5119117, 1.6223142, 1.391641 , 1.1503485,\n",
      "         1.1939363, 1.1233662, 1.0565788, 1.1294485],\n",
      "        [1.5306942, 2.833386 , 2.4506993, 2.0782595, 1.5404537,\n",
      "         1.9125915, 1.5315703, 1.3774502, 1.8127363],\n",
      "        [2.7291772, 4.2061644, 3.7055142, 2.8471732, 2.8603098,\n",
      "         3.5946646, 2.8656385, 3.143552 , 3.9485307],\n",
      "        [1.579748 , 2.0337079, 1.6026336, 1.169717 , 1.8671602,\n",
      "         2.2155626, 1.5847921, 2.2173414, 2.0998015]],\n",
      "\n",
      "       [[1.0056018, 1.0147829, 1.0316676, 1.0073755, 1.0037178,\n",
      "         1.0042491, 1.0068674, 1.0046245, 1.0061878],\n",
      "        [1.0141711, 1.1035784, 1.222798 , 1.041278 , 1.029009 ,\n",
      "         1.024745 , 1.0155784, 1.0179418, 1.0227755],\n",
      "        [1.1688275, 1.5119117, 1.6223142, 1.391641 , 1.1503485,\n",
      "         1.1939363, 1.1233662, 1.0565788, 1.1294485],\n",
      "        [1.5306942, 2.833386 , 2.4506993, 2.0782595, 1.5404537,\n",
      "         1.9125915, 1.5315703, 1.3774502, 1.8127363],\n",
      "        [2.7291772, 4.2061644, 3.7055142, 2.8471732, 2.8603098,\n",
      "         3.5946646, 2.8656385, 3.143552 , 3.9485307]]], dtype=float32)], [Array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)], [Array([1., 1., 1., 1., 1.], dtype=float32)], Array([1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32), Array([[0],\n",
      "       [1],\n",
      "       [2],\n",
      "       [3],\n",
      "       [4],\n",
      "       [5],\n",
      "       [6],\n",
      "       [7],\n",
      "       [8]], dtype=int32))\n",
      "[Array([[[2.7291772, 4.2061644, 3.7055142, 2.8471732, 2.8603098,\n",
      "         3.5946646, 2.8656385, 3.143552 , 3.9485307],\n",
      "        [1.579748 , 2.0337079, 1.6026336, 1.169717 , 1.8671602,\n",
      "         2.2155623, 1.584792 , 2.2173414, 2.0998015],\n",
      "        [1.1514773, 1.1519866, 1.1219418, 1.0301282, 1.381723 ,\n",
      "         1.3801522, 1.2883446, 1.4338273, 1.3446121],\n",
      "        [1.0247917, 1.0246316, 1.0208081, 1.0085815, 1.0982224,\n",
      "         1.0807873, 1.1387777, 1.0457131, 1.0427959],\n",
      "        [1.0027877, 1.0059218, 1.0040566, 1.0010021, 1.0120908,\n",
      "         1.0078646, 1.0202205, 1.0138859, 1.007665 ]],\n",
      "\n",
      "       [[1.5306942, 2.833386 , 2.4506993, 2.0782597, 1.5404537,\n",
      "         1.9125915, 1.5315704, 1.3774502, 1.8127363],\n",
      "        [2.7291772, 4.2061644, 3.7055142, 2.8471732, 2.8603098,\n",
      "         3.5946646, 2.8656385, 3.143552 , 3.9485307],\n",
      "        [1.579748 , 2.0337079, 1.6026336, 1.169717 , 1.8671602,\n",
      "         2.2155626, 1.5847921, 2.2173414, 2.0998015],\n",
      "        [1.1514773, 1.1519866, 1.1219418, 1.0301282, 1.381723 ,\n",
      "         1.3801522, 1.2883446, 1.4338273, 1.3446121],\n",
      "        [1.0247917, 1.0246316, 1.0208081, 1.0085815, 1.0982224,\n",
      "         1.0807873, 1.1387777, 1.0457131, 1.0427959]],\n",
      "\n",
      "       [[1.1688275, 1.5119117, 1.6223142, 1.391641 , 1.1503485,\n",
      "         1.1939363, 1.1233662, 1.0565788, 1.1294485],\n",
      "        [1.5306942, 2.833386 , 2.4506993, 2.0782595, 1.5404537,\n",
      "         1.9125915, 1.5315703, 1.3774502, 1.8127363],\n",
      "        [2.7291772, 4.2061644, 3.7055142, 2.8471732, 2.8603098,\n",
      "         3.5946646, 2.8656385, 3.143552 , 3.9485307],\n",
      "        [1.579748 , 2.0337079, 1.6026336, 1.169717 , 1.8671602,\n",
      "         2.2155626, 1.5847921, 2.2173414, 2.0998015],\n",
      "        [1.1514773, 1.1519866, 1.1219418, 1.0301282, 1.381723 ,\n",
      "         1.3801522, 1.2883446, 1.4338273, 1.3446121]],\n",
      "\n",
      "       [[1.0141711, 1.1035784, 1.222798 , 1.041278 , 1.029009 ,\n",
      "         1.024745 , 1.0155784, 1.0179418, 1.0227755],\n",
      "        [1.1688275, 1.5119117, 1.6223142, 1.391641 , 1.1503485,\n",
      "         1.1939363, 1.1233662, 1.0565788, 1.1294485],\n",
      "        [1.5306942, 2.833386 , 2.4506993, 2.0782595, 1.5404537,\n",
      "         1.9125915, 1.5315703, 1.3774502, 1.8127363],\n",
      "        [2.7291772, 4.2061644, 3.7055142, 2.8471732, 2.8603098,\n",
      "         3.5946646, 2.8656385, 3.143552 , 3.9485307],\n",
      "        [1.579748 , 2.0337079, 1.6026336, 1.169717 , 1.8671602,\n",
      "         2.2155626, 1.5847921, 2.2173414, 2.0998015]],\n",
      "\n",
      "       [[1.0056018, 1.0147829, 1.0316676, 1.0073755, 1.0037178,\n",
      "         1.0042491, 1.0068674, 1.0046245, 1.0061878],\n",
      "        [1.0141711, 1.1035784, 1.222798 , 1.041278 , 1.029009 ,\n",
      "         1.024745 , 1.0155784, 1.0179418, 1.0227755],\n",
      "        [1.1688275, 1.5119117, 1.6223142, 1.391641 , 1.1503485,\n",
      "         1.1939363, 1.1233662, 1.0565788, 1.1294485],\n",
      "        [1.5306942, 2.833386 , 2.4506993, 2.0782595, 1.5404537,\n",
      "         1.9125915, 1.5315703, 1.3774502, 1.8127363],\n",
      "        [2.7291772, 4.2061644, 3.7055142, 2.8471732, 2.8603098,\n",
      "         3.5946646, 2.8656385, 3.143552 , 3.9485307]]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The environment is statically defined by its HMM matrices : \n",
    "from simulate.hmm_weights import basic_latent_model\n",
    "\n",
    "from actynf.jaxtynf.layer_trial import compute_step_posteriors\n",
    "from actynf.jaxtynf.layer_learn import learn_after_trial\n",
    "from actynf.jaxtynf.layer_options import get_learning_options,get_planning_options\n",
    "\n",
    "from actynf.jaxtynf.shape_tools import to_log_space,get_vectorized_novelty\n",
    "\n",
    "\n",
    "def active_inference_basic_1D(hyperparameters,constants):\n",
    "    a0,b0,c0,d0,e0,u = basic_latent_model({**constants, **hyperparameters})\n",
    "    beta = hyperparameters[\"action_selection_temperature\"]\n",
    "    \n",
    "    planning_options = get_planning_options(constants[\"Th\"],\"classic\",a_novel=False,b_novel=False)\n",
    "    learning_options = get_learning_options(learn_b=True,lr_b=hyperparameters[\"transition_learning_rate\"],method=\"vanilla+backwards\",\n",
    "                                    state_generalize_function=lambda x : jnp.exp(-hyperparameters[\"state_interpolation_temperature\"]),\n",
    "                                    action_generalize_table=None,\n",
    "                                    cross_action_extrapolation_coeff=None)\n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        # The initial parameters of the AIF agent are its model weights :\n",
    "        return a0,b0,c0,d0,e0,u\n",
    "    \n",
    "    def initial_state(params):\n",
    "        pa,pb,pc,pd,pe,u = params\n",
    "\n",
    "        # The \"states\" of the active Inference agent are : \n",
    "        # 1. The vectorized parameters for this trial :\n",
    "        trial_a,trial_b,trial_d = vectorize_weights(pa,pb,pd,u)\n",
    "        trial_c,trial_e = to_log_space(pc,pe)\n",
    "        trial_a_nov,trial_b_nov = get_vectorized_novelty(pa,pb,u,compute_a_novelty=True,compute_b_novelty=True)\n",
    "        \n",
    "        # 2. Its priors about the next state : (given by the d matrix parameter)\n",
    "        prior = trial_d\n",
    "        \n",
    "        return prior,(trial_a,trial_b,trial_c,trial_e,trial_a_nov,trial_b_nov) # We don't need trial_d anymore !\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        emission,reward,trial_over,t = observation\n",
    "        gauge_level = emission[0]\n",
    "                \n",
    "        state_prior,timestep_weights = state\n",
    "        a_norm,b_norm,c,e,a_novel,b_novel = timestep_weights\n",
    "        \n",
    "        end_of_trial_filter = jnp.ones((planning_options[\"horizon\"]+2,))\n",
    "        qs,F,raw_qpi,efe = compute_step_posteriors(t,state_prior,emission,a_norm,b_norm,c,e,a_novel,b_novel,\n",
    "                                    end_of_trial_filter,\n",
    "                                    rng_key,planning_options)       \n",
    "\n",
    "        # Action selection :        \n",
    "        action_distribution = jax.nn.softmax(beta*efe)\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0])  \n",
    "        \n",
    "        # New state prior : \n",
    "        new_prior = jnp.einsum(\"iju,j,u->i\",b_norm,qs,vect_action_selected)\n",
    "        \n",
    "        # OPTIONAL : ONLINE UPDATING OF PARAMETERS \n",
    "        \n",
    "        return (new_prior,timestep_weights),(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        pa,pb,pc,pd,pe,u = params\n",
    "        rewards,observations,states,actions = trial_history\n",
    "        \n",
    "        posteriors_history = [s[0] for s in states]        \n",
    "        \n",
    "        gauge_levels_only = [o[0] for o in observations]\n",
    "        \n",
    "        # reward_array = jnp.stack(rewards)\n",
    "        obs_vect_arr = [jnp.stack(gauge_levels_only)]\n",
    "        qs_arr = jnp.stack(posteriors_history)\n",
    "        u_vect_arr = jnp.stack(actions)    \n",
    "        \n",
    "        # Then, we update the parameters of our HMM model at this level\n",
    "        # We use the raw weights here !\n",
    "        a_post,b_post,c_post,d_post,e_post,qs_post = learn_after_trial(obs_vect_arr,qs_arr,u_vect_arr,\n",
    "                                                pa,pb,c,pd,e,u,\n",
    "                                                method = learning_options[\"method\"],\n",
    "                                                learn_what = learning_options[\"bool\"],\n",
    "                                                learn_rates = learning_options[\"rates\"],\n",
    "                                                generalize_state_function=learning_options[\"state_generalize_function\"],\n",
    "                                                generalize_action_table=learning_options[\"action_generalize_table\"],\n",
    "                                                cross_action_extrapolation_coeff=learning_options[\"cross_action_extrapolation_coeff\"],\n",
    "                                                em_iter = learning_options[\"em_iterations\"])\n",
    "        \n",
    "        # The params for the next step is the last choice kernel of the trial :\n",
    "        # (the update already occured during the actor step !)\n",
    "        return a_post,b_post,c_post,d_post,e_post,u\n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    return initial_params,initial_state,actor_step,update_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We get a model weights by defining a \"parameters\" object :\n",
    "aif_1d_constants = {\n",
    "    # General environment : \n",
    "    \"N_feedback_ticks\":preprocessing_options[\"observations\"][\"N_bins\"],\n",
    "    # Latent state space structure\n",
    "    \"Ns_latent\":5,      # For 1D\n",
    "    # Action discretization:\n",
    "    \"N_actions_distance\" :3,\n",
    "    \"N_actions_position\" :9,\n",
    "    \"N_actions_angle\" :9,\n",
    "    \n",
    "    \"Th\" : 3\n",
    "}\n",
    "\n",
    "aif_1d_params = {    \n",
    "    # ----------------------------------------------------------------------------------------------------\n",
    "    # Model parameters : these should interact with the model components in a differentiable manner\n",
    "    \"transition_concentration\": 1.0,\n",
    "    \"transition_stickiness\": 1.0,\n",
    "    \"transition_learning_rate\" : 1.0,\n",
    "    \"state_interpolation_temperature\" : 1.0,\n",
    "    \n",
    "    \"initial_state_concentration\": 1.0,\n",
    "    \n",
    "    \"feedback_expected_std\" : 0.15,\n",
    "    \"emission_concentration\" : 1.0,\n",
    "    \"emission_stickiness\" : 100.0,\n",
    "    \n",
    "    \"reward_seeking\" : 10.0,\n",
    "    \n",
    "    \"action_selection_temperature\" : 1.0,\n",
    "}\n",
    "\n",
    "a0,b0,c0,d0,e0,u = basic_latent_model({**aif_1d_constants, **aif_1d_params})\n",
    "p = run_loop(env,active_inference_basic_1D(aif_1d_params,aif_1d_constants),SEED,NTRIALS)\n",
    "print(p)\n",
    "\n",
    "a,b,c,d,e,u = p\n",
    "\n",
    "\n",
    "nu = u.shape[0]\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(2,nu)\n",
    "print(b)\n",
    "for act in range(nu):\n",
    "    ax = axs[0,act]\n",
    "    ax.imshow(b0[0][...,act])\n",
    "    \n",
    "    ax = axs[1,act]\n",
    "    ax.imshow(b[0][...,act])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nice, we have all of our proposal models.  However, we aim at performing model inversion based on task data ! This means that we're going to need **likelihood functions** for each of these models !\n",
    "\n",
    "Likelihood function describe the probability of these models generating the observed actions, given their hyperparameters $\\theta$ and their previous experiences $o_{1:T,1:t},s_{1:T,1:t}$ : \n",
    "$$\n",
    "\\sum_T \\sum_{t\\in T} P(u_t|o_{1:T,1:t},u_{1:T,1:t-1},\\theta)\n",
    "$$\n",
    "\n",
    "Let's add likelihood functions for each of our proposal models. These functions depend on the data of the task (sequences of observations $o$ and performed actions $u$) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(hyperparameters,constants):\n",
    "    # a,b,c = hyperparameters\n",
    "    num_actions, = constants\n",
    "    \n",
    "    \n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        return None # A function of the hyperparameters\n",
    "    \n",
    "    def initial_state(params):\n",
    "        # Initial agent state (beginning of each trial)\n",
    "        return None\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        gauge_level,reward,trial_over,t = observation\n",
    "        \n",
    "        # OPTIONAL : Update states based on previous states, observations and parameters\n",
    "        new_state = state\n",
    "        \n",
    "        # Compute action distribution using observation, states and parameters\n",
    "        action_distribution,_ = _normalize(jnp.ones((num_actions,)))\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0])\n",
    "        \n",
    "        return new_state,(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        rewards,observations,states,actions = trial_history\n",
    "        \n",
    "        # Trial history is a list of trial rewards, observations and states, we may want to make them jnp arrays :\n",
    "        # reward_array = jnp.stack(rewards)\n",
    "        # observation_array = jnp.stack(observations)\n",
    "        # states_array = jnp.stack(states)\n",
    "        # action_array = jnp.stack(actions)\n",
    "        \n",
    "        # OPTIONAL :  Update parameters based on states, observations and actions history\n",
    "        return None\n",
    "    \n",
    "    def predict(observation,state,params,true_action):\n",
    "        \"\"\"Predict the next action given a set of observations,\n",
    "        as well as the previous internal states and parameters of the agent.\n",
    "\n",
    "        Args:\n",
    "            observation (_type_): _description_\n",
    "            state (_type_): _description_\n",
    "            params (_type_): _description_\n",
    "            true_action : the actual action that was performed (for state updating purposes !)\n",
    "\n",
    "        Returns:\n",
    "            new_state : the \n",
    "            predicted_action : $P(u_t|o_t,s_{t-1},\\theta)$\n",
    "        \"\"\"\n",
    "        gauge_level,reward,trial_over,t = observation\n",
    "        \n",
    "        # OPTIONAL : Update states based on previous states, observations and parameters\n",
    "        new_state = state\n",
    "        \n",
    "        # Compute action distribution using observation, states and parameters\n",
    "        predicted_action,_ = _normalize(jnp.ones((num_actions,)))\n",
    "        return new_state,predicted_action\n",
    "        \n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    return initial_params,initial_state,actor_step,update_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the LL of the blind model : \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploit_results_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
