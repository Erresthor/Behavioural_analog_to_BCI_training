{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational modeling : likelihood for various RL agents\n",
    "\n",
    "Question : under very low amounts of evidence, how do human sample a complex action space ? Can we infer some form of structure in this exploration ? Can Active Inference provide some answers regarding the mechanistic processes behind it ?\n",
    "\n",
    "In notebook 103, we derived a few proposal models to explain the behaviour of our subjects.  However, this is not enough as we aim at performing model inversion based on task data ! This means that we're going to need **likelihood functions** for each of these models !\n",
    "\n",
    "Likelihood function describe the probability of these models generating the observed actions, given their hyperparameters $\\theta$ and their previous experiences $o_{1:T,1:t},s_{1:T,1:t}$ : \n",
    "$$\n",
    "\\prod_T \\prod_{t\\in T} P(u_t|o_{1:T,1:t},u_{1:T,1:t-1},\\theta)\n",
    "$$\n",
    "\n",
    "In this notebook, we modify the previous models to compute their likelihood in a jax environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annic\\OneDrive\\Bureau\\MainPhD\\code\\behavioural_exp_code\\exploit_results_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the needed packages \n",
    "# \n",
    "# 1/ the usual suspects\n",
    "import sys,os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax import vmap\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# 2/ The Active Inference package \n",
    "import actynf\n",
    "from actynf.jaxtynf.jax_toolbox import _normalize,_jaxlog\n",
    "from actynf.jaxtynf.layer_trial import compute_step_posteriors\n",
    "from actynf.jaxtynf.layer_learn import learn_after_trial\n",
    "from actynf.jaxtynf.layer_options import get_learning_options,get_planning_options\n",
    "from actynf.jaxtynf.shape_tools import to_log_space,get_vectorized_novelty\n",
    "\n",
    "from actynf.jaxtynf.layer_process import initial_state_and_obs,process_update\n",
    "from actynf.jaxtynf.shape_tools import vectorize_weights\n",
    "\n",
    "\n",
    "# 3/ Tools for : \n",
    "# a. Getting the raw data : \n",
    "from database_handling.database_extract import get_all_subject_data_from_internal_task_id\n",
    "from utils import remove_by_indices\n",
    "# b. Preprocessing the data :\n",
    "from analysis_tools.preprocess import OPTIONS_PREPROCESS_DEFAULT,get_preprocessed_data\n",
    "\n",
    "\n",
    "\n",
    "# The environment is statically defined by its HMM matrices : \n",
    "from hmm_weights import behavioural_process\n",
    "# Weights for the active inference model : \n",
    "from hmm_weights import basic_latent_model\n",
    "\n",
    "\n",
    "# To generate synthetic data :\n",
    "from simulate.generate_observations import TrainingEnvironment,run_loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantitate some agents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(hyperparameters,constants):\n",
    "    # a,b,c = hyperparameters\n",
    "    num_actions, = constants\n",
    "    \n",
    "    \n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        return None # A function of the hyperparameters\n",
    "    \n",
    "    def initial_state(params):\n",
    "        # Initial agent state (beginning of each trial)\n",
    "        return None\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        gauge_level,reward,trial_over,t = observation\n",
    "        \n",
    "        # OPTIONAL : Update states based on previous states, observations and parameters\n",
    "        new_state = state\n",
    "        \n",
    "        # Compute action distribution using observation, states and parameters\n",
    "        action_distribution,_ = _normalize(jnp.ones((num_actions,)))\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0])\n",
    "        \n",
    "        return new_state,(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        rewards,observations,states,actions = trial_history\n",
    "        \n",
    "        # Trial history is a list of trial rewards, observations and states, we may want to make them jnp arrays :\n",
    "        # reward_array = jnp.stack(rewards)\n",
    "        # observation_array = jnp.stack(observations)\n",
    "        # states_array = jnp.stack(states)\n",
    "        # action_array = jnp.stack(actions)\n",
    "        \n",
    "        # OPTIONAL :  Update parameters based on states, observations and actions history\n",
    "        return None\n",
    "    \n",
    "    def predict(data_timestep,state,params):\n",
    "        \"\"\"Predict the next action given a set of observations,\n",
    "        as well as the previous internal states and parameters of the agent.\n",
    "\n",
    "        Args:\n",
    "            observation (_type_): _description_\n",
    "            state (_type_): _description_\n",
    "            params (_type_): _description_\n",
    "            true_action : the actual action that was performed (for state updating purposes !)\n",
    "\n",
    "        Returns:\n",
    "            new_state : the \n",
    "            predicted_action : $P(u_t|o_t,s_{t-1},\\theta)$\n",
    "        \"\"\"\n",
    "        gauge_level,obs_bool_filter,reward,true_action,t = data_timestep\n",
    "        \n",
    "        # OPTIONAL : Update states based on previous states, observations and parameters\n",
    "        new_state = state\n",
    "        \n",
    "        # Compute action distribution using observation, states and parameters\n",
    "        predicted_action,_ = _normalize(jnp.ones((num_actions,)))\n",
    "        \n",
    "        # Here are the data we may want to report during the training : \n",
    "        other_data = None\n",
    "        \n",
    "        return new_state,predicted_action,other_data\n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    return initial_params,initial_state,actor_step,update_params,predict,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some synthetic data for this agent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n"
     ]
    }
   ],
   "source": [
    "# The synthetic environment CONSTANTS :\n",
    "T = 11\n",
    "N_FEEDBACK_OUTCOMES = 10\n",
    "TRUE_FEEDBACK_STD = 0.15\n",
    "GRID_SIZE = (7,7)\n",
    "START_COORD = [[5,1],[5,2],[4,1]]\n",
    "END_COORD = [0,6]\n",
    "(a,b,c,d,e,u),fb_vals = behavioural_process(GRID_SIZE,START_COORD,END_COORD,N_FEEDBACK_OUTCOMES,TRUE_FEEDBACK_STD)\n",
    "rngkey = jax.random.PRNGKey(np.random.randint(0,10))\n",
    "ENVIRONMENT = TrainingEnvironment(rngkey,a,b,c,d,e,u,T)\n",
    "\n",
    "\n",
    "# In : an agent based on some hyperparameters : \n",
    "SEED = 100\n",
    "NTRIALS = 10\n",
    "random_agent_hyperparameters = None\n",
    "random_agent_constants = (9,)\n",
    "\n",
    "# Synthetic data (here, generated randomly) :\n",
    "params_final,training_hist = run_loop(ENVIRONMENT,random_agent(random_agent_hyperparameters,random_agent_constants),SEED,NTRIALS)\n",
    "# Parameter update (once every trial)\n",
    "def _swaplist(_list):\n",
    "    \"\"\" Put the various factors / modalities as the leading dimension for a 2D list of lists.\"\"\"\n",
    "    if _list is None :\n",
    "        return None\n",
    "    \n",
    "    for el in _list :\n",
    "        if (type(el) != list) and (type(el) != tuple):\n",
    "            # There is a single factor here ! \n",
    "            return _list\n",
    "    \n",
    "    _swapped_list = []\n",
    "    for factor in range(len(_list[0])):\n",
    "        _swapped_list.append([_el[factor] for _el in _list])\n",
    "    return _swapped_list\n",
    "        \n",
    "        \n",
    "formatted_stimuli= [jnp.array(o) for o in _swaplist(training_hist[\"stimuli\"])]\n",
    "bool_stimuli = [jnp.ones_like(stim[...,0]) for stim in formatted_stimuli]\n",
    "rewards = jnp.array(training_hist[\"rewards\"])\n",
    "actions = jnp.array(training_hist[\"actions\"])\n",
    "tmtsp = jnp.array(training_hist[\"timestamps\"])\n",
    "synthetic_data = (formatted_stimuli,bool_stimuli,rewards,actions,tmtsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]]\n",
      "-2.1972246\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_predicted_actions(data,agent_functions):\n",
    "    \"\"\"A function that uses vmap to compute the predicted agent action at time $t$ given $o_{1:t}$ and $u_{1:t-1}$. \n",
    "    This function should be differentiable w.r.t. the hyperparameters of the agent's model because we're going to perform\n",
    "    gradient descent on it !\n",
    "\n",
    "    Args:\n",
    "        environment (_type_): _description_\n",
    "        agent_functions (_type_): _description_\n",
    "        seed (_type_): _description_\n",
    "        Ntrials (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    init_params,init_state,_,agent_learn,predict,_ = agent_functions\n",
    "    \n",
    "    \n",
    "    # Data should contain :\n",
    "    # - all observations -> stimuli,reward (from the system)\n",
    "    #       -> a list of stimuli for each modality\n",
    "    #       -> a list of observation filters for each modality\n",
    "    #       -> a Ntrials x Ntimesteps tensor array of scalar rewards (\\in [0,1])\n",
    "    # - all true actions \n",
    "    #       -> a Ntrials x (Ntimesteps-1) x Nu tensor array encoding the observed actions\n",
    "    #       -> a Ntrials x (Ntimesteps-1) filter tensor indicating which actions were NOT observed\n",
    "    \n",
    "    initial_parameters = init_params()  \n",
    "        # The initial parameters of the tested model are initialized once per training\n",
    "    \n",
    "    \n",
    "    def _scan_trial(_carry,_data_trial):\n",
    "        \n",
    "        _agent_params = _carry\n",
    "        _initial_state = init_state(_agent_params)\n",
    "        \n",
    "        _observations_trial,_observations_filter_trial,_rewards_trial,_actions_trial,_timestamps_trial = _data_trial\n",
    "        \n",
    "        # The same actions, with an extra one at the end for scan to work better !\n",
    "        _expanded_actions_trial = jnp.concatenate([_actions_trial,jnp.zeros((1,_actions_trial.shape[-1]))])\n",
    "        _expanded_data_trial = (_observations_trial,_observations_filter_trial,_rewards_trial,_expanded_actions_trial,_timestamps_trial)\n",
    "        \n",
    "        def __scan_timestep(__carry,__data_timestep):\n",
    "            # __obs_vect,__obs_bool,__reward,__true_action_vect,__t = __data_timestep\n",
    "            __agent_state = __carry\n",
    "                    \n",
    "            __new_state,__predicted_action,__other_data = predict(__data_timestep,__agent_state,_agent_params)        \n",
    "            \n",
    "            return __new_state,(__predicted_action,__new_state,__other_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        _,(_predicted_actions,_trial_states,_trial_other_data) = jax.lax.scan(__scan_timestep, (_initial_state),_expanded_data_trial)\n",
    "          \n",
    "        \n",
    "        _new_params = agent_learn((_rewards_trial,_observations_trial,_trial_states,_actions_trial),_agent_params)\n",
    "        \n",
    "        return _new_params,(_predicted_actions[:-1,...],(_trial_states,_trial_other_data))\n",
    "\n",
    "    final_parameters,(predicted_actions,(model_states,other_data)) = jax.lax.scan(_scan_trial,initial_parameters,data)\n",
    "\n",
    "    return final_parameters,predicted_actions,(model_states,other_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# And compute the likelihood of each action given the random model : \n",
    "final_parameters,predicted_actions,model_states = compute_predicted_actions(synthetic_data,random_agent(random_agent_hyperparameters,random_agent_constants))\n",
    "print(predicted_actions)\n",
    "# print(model_states)\n",
    "\n",
    "# Here's the average log-likelihood of what was observed given this model :\n",
    "avg_ll = jnp.mean((actions * _jaxlog(predicted_actions)).sum(axis=-1))\n",
    "print(avg_ll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 11, 9)\n",
      "[9.7656250e-03 1.0768622e-03 4.0283222e-03 2.5001526e-01 2.4414156e-04\n",
      " 4.8834115e-04 5.0000000e-01 5.7220791e-06 2.3437572e-01]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def choice_kernel_agent(hyperparameters,constants):\n",
    "    alpha,beta = hyperparameters\n",
    "    num_actions, = constants\n",
    "\n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        # Parameters is the initial choice kernel :\n",
    "        CK_initial = jnp.zeros((num_actions,))\n",
    "        \n",
    "        return CK_initial # A function of the hyperparameters\n",
    "    \n",
    "    def initial_state(params):\n",
    "        # Initial agent state (beginning of each trial)\n",
    "        # The initial state is the CK table and an initial action (easier integration with rw+ck model)\n",
    "        return params,jnp.zeros((num_actions,))\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        gauge_level,reward,trial_over,t = observation\n",
    "        \n",
    "        # The state of the agent stores the choice kernel and the last action performed : \n",
    "        ck,last_action = state\n",
    "        \n",
    "        # Update the choice kernel :\n",
    "        was_a_last_action = jnp.sum(last_action)  # No update if there was no last action\n",
    "        new_ck = ck + alpha*(last_action - ck)*was_a_last_action\n",
    "        \n",
    "        action_distribution = jax.nn.softmax(beta*ck)\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0])        \n",
    "        \n",
    "        return (new_ck,vect_action_selected),(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        rewards,observations,states,actions = trial_history\n",
    "        \n",
    "        # The params for the next step is the last choice kernel of the trial :\n",
    "        # (the update already occured during the actor step !)\n",
    "        cks,previous_actions = states\n",
    "        \n",
    "        ck_last = cks[-1]\n",
    "        \n",
    "        return ck_last\n",
    "    \n",
    "    def predict(data_timestep,state,params):\n",
    "        \"\"\"Predict the next action given a set of observations,\n",
    "        as well as the previous internal states and parameters of the agent.\n",
    "\n",
    "        Args:\n",
    "            observation (_type_): _description_\n",
    "            state (_type_): _description_\n",
    "            params (_type_): _description_\n",
    "            true_action : the actual action that was performed (for state updating purposes !)\n",
    "\n",
    "        Returns:\n",
    "            new_state : the \n",
    "            predicted_action : $P(u_t|o_t,s_{t-1},\\theta)$\n",
    "        \"\"\"\n",
    "        gauge_level,obs_bool_filter,reward,true_action,t = data_timestep        \n",
    "        \n",
    "        # The state of the agent stores the choice kernel and the last action performed : \n",
    "        ck,last_action = state\n",
    "        \n",
    "        # Update the choice kernel :\n",
    "        was_a_last_action = jnp.sum(last_action)  # No update if there was no last action\n",
    "        new_ck = ck + alpha*(last_action - ck)*was_a_last_action\n",
    "        \n",
    "        predicted_action = jax.nn.softmax(beta*ck) \n",
    "        \n",
    "        # Here are the data we may want to report during the training : \n",
    "        other_data = None\n",
    "                \n",
    "        return (new_ck,true_action),predicted_action,other_data\n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    return initial_params,initial_state,actor_step,update_params,predict,None\n",
    "\n",
    "\n",
    "ck_agent_hyperparameters = (0.5,1.0)   # [0,1] x [0, +oo]\n",
    "ck_agent_constants = (9,)              # Nactions\n",
    "final_parameters,predicted_actions,state_history = compute_predicted_actions(synthetic_data,choice_kernel_agent(ck_agent_hyperparameters,ck_agent_constants))\n",
    "# print(predicted_actions)\n",
    "\n",
    "\n",
    "# We can have an idea of what happened during training by looking at the inner states of the model\n",
    "inner_states,_ = state_history\n",
    "# Of course, these will vary from one model to the next :\n",
    "ck_table,previous_action = inner_states\n",
    "print(ck_table.shape)\n",
    "print(ck_table[-1,-1,:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11303929 0.11303929 0.11303929 0.11303929 0.11303929 0.11303929\n",
      "   0.11303929 0.0956857  0.11303929]\n",
      "  [0.10994653 0.10994653 0.1373066  0.10994653 0.10994653 0.10994653\n",
      "   0.10994653 0.09306773 0.10994653]\n",
      "  [0.11231401 0.11231401 0.11873025 0.11231401 0.11231401 0.11231401\n",
      "   0.11231401 0.09507176 0.11231401]\n",
      "  [0.11365641 0.11365641 0.12014934 0.11365641 0.11365641 0.11365641\n",
      "   0.10170422 0.09620807 0.11365641]\n",
      "  [0.11327305 0.11327305 0.12311694 0.11327305 0.11327305 0.11327305\n",
      "   0.10136118 0.09588357 0.11327305]\n",
      "  [0.11099289 0.11099289 0.12063864 0.11099289 0.11099289 0.11099289\n",
      "   0.09932081 0.09395346 0.13112262]\n",
      "  [0.11099289 0.11099289 0.12063864 0.11099289 0.11099289 0.11099289\n",
      "   0.09932081 0.09395346 0.13112262]\n",
      "  [0.11350322 0.09088627 0.12336712 0.11350322 0.11350322 0.11350322\n",
      "   0.10156715 0.0960784  0.13408822]\n",
      "  [0.11038536 0.08838969 0.1199783  0.11038536 0.13785464 0.11038536\n",
      "   0.09877716 0.0934392  0.1304049 ]]\n",
      "\n",
      " [[0.11350322 0.09088627 0.12336712 0.11350322 0.11350322 0.11350322\n",
      "   0.10156715 0.0960784  0.13408822]\n",
      "  [0.11190131 0.08960356 0.12162599 0.11190131 0.11190131 0.11190131\n",
      "   0.10013369 0.10883571 0.13219579]\n",
      "  [0.11258212 0.09014872 0.12236597 0.11258212 0.11258212 0.10649813\n",
      "   0.10074291 0.10949788 0.13300008]\n",
      "  [0.11258212 0.09014872 0.12236597 0.11258212 0.11258212 0.10649813\n",
      "   0.10074291 0.10949788 0.13300008]\n",
      "  [0.11393098 0.0912288  0.12383205 0.11393098 0.10194992 0.10777409\n",
      "   0.10194992 0.11080978 0.13459356]\n",
      "  [0.11450966 0.09169217 0.1193817  0.11450966 0.10246775 0.1083215\n",
      "   0.10246775 0.11137262 0.1352772 ]\n",
      "  [0.11190784 0.08960879 0.11666918 0.11190784 0.10013954 0.1285817\n",
      "   0.10013954 0.10884207 0.13220349]\n",
      "  [0.11190784 0.08960879 0.11666918 0.11190784 0.10013954 0.1285817\n",
      "   0.10013954 0.10884207 0.13220349]\n",
      "  [0.11288159 0.09038851 0.11768436 0.11288159 0.10101089 0.12099919\n",
      "   0.10101089 0.10978915 0.13335384]\n",
      "  [0.11288159 0.09038851 0.11768436 0.11288159 0.10101089 0.12099919\n",
      "   0.10101089 0.10978915 0.13335384]]\n",
      "\n",
      " [[0.11180765 0.08952857 0.11656473 0.11180765 0.10004989 0.12936184\n",
      "   0.10004989 0.10874463 0.13208514]\n",
      "  [0.11180765 0.08952857 0.11656473 0.11180765 0.10004989 0.12936184\n",
      "   0.10004989 0.10874463 0.13208514]\n",
      "  [0.11221371 0.08985371 0.11698806 0.11221371 0.10041323 0.12983164\n",
      "   0.10041323 0.10913955 0.12893315]\n",
      "  [0.11282595 0.09034395 0.11762634 0.11282595 0.10096109 0.13054\n",
      "   0.0955051  0.10973502 0.1296366 ]\n",
      "  [0.11282595 0.09034395 0.11762634 0.11282595 0.10096109 0.13054\n",
      "   0.0955051  0.10973502 0.1296366 ]\n",
      "  [0.11224662 0.09501472 0.11702237 0.11224662 0.10044269 0.12986971\n",
      "   0.09501472 0.10917156 0.12897097]\n",
      "  [0.11249808 0.09522757 0.11728453 0.11249808 0.10066771 0.12792042\n",
      "   0.09522757 0.10941614 0.1292599 ]\n",
      "  [0.11269901 0.09539765 0.117494   0.11269901 0.10084751 0.1281489\n",
      "   0.09539765 0.10961156 0.12770471]\n",
      "  [0.11269901 0.09539765 0.117494   0.11269901 0.10084751 0.1281489\n",
      "   0.09539765 0.10961156 0.12770471]\n",
      "  [0.10972594 0.11926158 0.11439444 0.10972594 0.09818709 0.12476826\n",
      "   0.092881   0.10671994 0.12433579]]\n",
      "\n",
      " [[0.1105469  0.12015388 0.11525033 0.1105469  0.09892172 0.12570176\n",
      "   0.08609402 0.10751841 0.12526606]\n",
      "  [0.11179134 0.11024942 0.11654771 0.11179134 0.10003529 0.1271168\n",
      "   0.08706319 0.10872875 0.12667619]\n",
      "  [0.11100206 0.1165313  0.11572485 0.11100206 0.09932901 0.12621932\n",
      "   0.0864485  0.1079611  0.12578182]\n",
      "  [0.113968   0.11964498 0.11881699 0.113968   0.10198305 0.10287216\n",
      "   0.08875837 0.1108458  0.12914267]\n",
      "  [0.1144859  0.12018867 0.11935692 0.1144859  0.10244648 0.10333964\n",
      "   0.08916171 0.10680526 0.12972952]\n",
      "  [0.11250098 0.11810488 0.11728755 0.11250098 0.1006703  0.10154797\n",
      "   0.10495351 0.10495351 0.12748031]\n",
      "  [0.11067723 0.11619029 0.11538621 0.11067723 0.09903835 0.09990179\n",
      "   0.10325211 0.11946304 0.12541375]\n",
      "  [0.11094116 0.11646736 0.11327668 0.11094116 0.09927452 0.10014002\n",
      "   0.10349834 0.11974792 0.12571281]\n",
      "  [0.11153956 0.11709557 0.11388768 0.11153956 0.09441619 0.10068015\n",
      "   0.10405658 0.12039382 0.12639087]\n",
      "  [0.12915576 0.11477382 0.11162954 0.10932798 0.09254413 0.0986839\n",
      "   0.10199338 0.11800668 0.12388483]]\n",
      "\n",
      " [[0.12926657 0.11487231 0.11172532 0.1094218  0.09262355 0.09876858\n",
      "   0.1020809  0.11810794 0.12313307]\n",
      "  [0.12775446 0.11352858 0.12211598 0.10814182 0.09154007 0.09761322\n",
      "   0.1008868  0.11672636 0.1216927 ]\n",
      "  [0.12831575 0.11402736 0.12265249 0.10861693 0.09194224 0.09804208\n",
      "   0.10133004 0.11284574 0.12222736]\n",
      "  [0.11356603 0.11595681 0.12472789 0.11045483 0.09349799 0.09970104\n",
      "   0.10304464 0.1147552  0.12429556]\n",
      "  [0.11356603 0.11595681 0.12472789 0.11045483 0.09349799 0.09970104\n",
      "   0.10304464 0.1147552  0.12429556]\n",
      "  [0.11265071 0.11502222 0.1237226  0.10956459 0.10080428 0.09889746\n",
      "   0.10221411 0.11383029 0.12329375]\n",
      "  [0.12884958 0.11292244 0.12146401 0.10756445 0.09896407 0.09709206\n",
      "   0.10034816 0.11175228 0.12104298]\n",
      "  [0.1305605  0.11442187 0.12307685 0.10899273 0.10027815 0.09838129\n",
      "   0.10168061 0.11323617 0.10937183]\n",
      "  [0.13311592 0.11666142 0.1254858  0.11112601 0.10224087 0.10030688\n",
      "   0.10367078 0.09587981 0.11151253]\n",
      "  [0.13003647 0.11396262 0.12258286 0.10855526 0.12300923 0.09798642\n",
      "   0.10127251 0.09366176 0.10893285]]\n",
      "\n",
      " [[0.13156587 0.11530297 0.12402458 0.10983202 0.12445597 0.09913886\n",
      "   0.1024636  0.09476335 0.09845278]\n",
      "  [0.1297679  0.11372725 0.1359956  0.10833105 0.12275517 0.09778404\n",
      "   0.10106334 0.09346831 0.09710734]\n",
      "  [0.13111673 0.11490935 0.13740915 0.10945707 0.12403111 0.09880042\n",
      "   0.10211381 0.09443983 0.08772248]\n",
      "  [0.13116361 0.11495043 0.13710079 0.10949621 0.12407546 0.09883574\n",
      "   0.10215031 0.09447361 0.08775385]\n",
      "  [0.13127632 0.11504921 0.13721861 0.10959031 0.1233227  0.09892068\n",
      "   0.1022381  0.09455479 0.08782926]\n",
      "  [0.12131901 0.11636791 0.1387914  0.11084643 0.12473622 0.10005451\n",
      "   0.10340995 0.09563858 0.08883596]\n",
      "  [0.11830596 0.13831367 0.13534442 0.10809347 0.12163831 0.09756958\n",
      "   0.10084169 0.09326332 0.08662965]\n",
      "  [0.1187116  0.13878791 0.13580847 0.10846409 0.12205537 0.09790412\n",
      "   0.10118745 0.09015436 0.08692668]\n",
      "  [0.1187116  0.13878791 0.13580847 0.10846409 0.12205537 0.09790412\n",
      "   0.10118745 0.09015436 0.08692668]\n",
      "  [0.11960942 0.13983758 0.1292725  0.10928442 0.12297849 0.09864458\n",
      "   0.10195275 0.09083621 0.08758412]]\n",
      "\n",
      " [[0.11856627 0.138618   0.12814508 0.10833131 0.12190595 0.09778427\n",
      "   0.10106358 0.09876531 0.08682027]\n",
      "  [0.12563625 0.13750616 0.12711723 0.10746238 0.12092815 0.09699994\n",
      "   0.10025295 0.09797311 0.08612388]\n",
      "  [0.12564094 0.13751128 0.12712197 0.10746639 0.12093265 0.09700356\n",
      "   0.10025669 0.09797676 0.08608972]\n",
      "  [0.12357793 0.13525335 0.12503463 0.1057018  0.11894695 0.09541076\n",
      "   0.09861048 0.11278791 0.08467614]\n",
      "  [0.12428789 0.13603039 0.12575296 0.10056407 0.1196303  0.0959589\n",
      "   0.099177   0.11343588 0.0851626 ]\n",
      "  [0.12354243 0.1412124  0.12499871 0.09996091 0.11891277 0.09538335\n",
      "   0.09858215 0.11275551 0.08465181]\n",
      "  [0.12610963 0.1441468  0.12759618 0.10203809 0.12138377 0.09736542\n",
      "   0.10063068 0.09431861 0.08641087]\n",
      "  [0.12474108 0.1425825  0.1262115  0.10093077 0.12006651 0.1071608\n",
      "   0.09953863 0.09329506 0.08547313]\n",
      "  [0.12350309 0.14116745 0.12495891 0.09992908 0.11887491 0.10609729\n",
      "   0.09855076 0.09236915 0.09454937]\n",
      "  [0.1249226  0.14278997 0.12639515 0.10107763 0.12024122 0.09582306\n",
      "   0.09968347 0.09343082 0.09563608]]\n",
      "\n",
      " [[0.12448416 0.14228883 0.12595154 0.10072288 0.11981921 0.09548675\n",
      "   0.10284328 0.0931029  0.09530044]\n",
      "  [0.12612018 0.14415886 0.12760685 0.10204662 0.10825153 0.09674167\n",
      "   0.10419488 0.0943265  0.09655291]\n",
      "  [0.12903872 0.14749482 0.10741886 0.10440807 0.11075658 0.09898037\n",
      "   0.10660605 0.0965093  0.09878723]\n",
      "  [0.12718004 0.14537029 0.10587159 0.10290417 0.10916123 0.09755465\n",
      "   0.11947459 0.09511918 0.0973643 ]\n",
      "  [0.12867679 0.14708112 0.10711757 0.10411523 0.11044592 0.09870274\n",
      "   0.10911182 0.09623861 0.09851016]\n",
      "  [0.13128252 0.12980936 0.10928673 0.10622358 0.11268248 0.1007015\n",
      "   0.11132136 0.09818747 0.10050502]\n",
      "  [0.13336307 0.1160188  0.11101867 0.10790699 0.11446825 0.1022974\n",
      "   0.11308555 0.09974352 0.10209779]\n",
      "  [0.13260368 0.11535817 0.11038651 0.10729256 0.11381645 0.10740903\n",
      "   0.11244163 0.09917556 0.10151643]\n",
      "  [0.13097043 0.12625407 0.10902692 0.10597106 0.11241461 0.10608611\n",
      "   0.11105672 0.09795405 0.10026608]\n",
      "  [0.13097185 0.12625544 0.1090281  0.10597222 0.11241583 0.10608726\n",
      "   0.11105793 0.09795512 0.10025629]]\n",
      "\n",
      " [[0.13022673 0.12553714 0.10840782 0.10536931 0.11177627 0.1054837\n",
      "   0.1104261  0.09739783 0.10537503]\n",
      "  [0.13203241 0.11341222 0.10991096 0.10683033 0.11332612 0.1069463\n",
      "   0.11195722 0.09874831 0.10683612]\n",
      "  [0.13113354 0.11264011 0.1091627  0.10610303 0.11255459 0.10621822\n",
      "   0.11119502 0.10488396 0.10610878]\n",
      "  [0.12815395 0.11008073 0.10668232 0.10369218 0.10999715 0.10380474\n",
      "   0.10866847 0.12522262 0.1036978 ]\n",
      "  [0.12818377 0.10987362 0.10670715 0.10371631 0.11002275 0.10382891\n",
      "   0.10869376 0.12525177 0.10372194]\n",
      "  [0.12854902 0.1101867  0.10701121 0.10116239 0.11033626 0.10412476\n",
      "   0.10900348 0.12560867 0.10401749]\n",
      "  [0.12692203 0.12144871 0.10565681 0.09988201 0.10893977 0.1028069\n",
      "   0.10762387 0.12401889 0.10270098]\n",
      "  [0.12857148 0.1100312  0.10702991 0.10118006 0.11035554 0.10414296\n",
      "   0.10902253 0.12563062 0.10403566]\n",
      "  [0.12676992 0.10848942 0.11954244 0.09976231 0.10880921 0.10268369\n",
      "   0.10749488 0.12387025 0.10257789]\n",
      "  [0.12519543 0.10714199 0.11805773 0.09852326 0.11987783 0.10140835\n",
      "   0.10615979 0.12233178 0.10130387]]\n",
      "\n",
      " [[0.12686986 0.10857496 0.1196367  0.09984096 0.12148114 0.0893901\n",
      "   0.10757963 0.12396791 0.10265876]\n",
      "  [0.12609158 0.11404334 0.11890279 0.09922849 0.12073592 0.08884174\n",
      "   0.10691969 0.12320744 0.10202901]\n",
      "  [0.1120821  0.11587155 0.1208089  0.10081921 0.12267142 0.09026594\n",
      "   0.1086337  0.12518255 0.10366462]\n",
      "  [0.11199003 0.11577637 0.12153106 0.10073639 0.12257066 0.0901918\n",
      "   0.10854446 0.12507972 0.10357947]\n",
      "  [0.11087269 0.11592205 0.12168398 0.10086315 0.12272488 0.09030528\n",
      "   0.10868104 0.1252371  0.1037098 ]\n",
      "  [0.10918586 0.11415839 0.11983266 0.0993286  0.12085772 0.08893136\n",
      "   0.10702755 0.12333173 0.11734606]\n",
      "  [0.1103861  0.11541329 0.12114993 0.10042048 0.12218627 0.08990895\n",
      "   0.10820406 0.12468747 0.10764343]\n",
      "  [0.11030344 0.11532687 0.12105922 0.10034528 0.12209477 0.08984163\n",
      "   0.10812303 0.1245941  0.10831165]\n",
      "  [0.11026193 0.11528347 0.12101366 0.10030752 0.12204883 0.08980782\n",
      "   0.10808235 0.12454722 0.10864712]\n",
      "  [0.11041469 0.11544319 0.12118132 0.09906104 0.12221792 0.08993224\n",
      "   0.1082321  0.12471978 0.10879764]]]\n",
      "(10, 11, 9)\n",
      "[ 0.01128472  0.05582005  0.10432944 -0.09722222  0.11284722 -0.19390193\n",
      " -0.05989584  0.1331109  -0.00346883]\n"
     ]
    }
   ],
   "source": [
    "def rescorla_wagner_agent(hyperparameters,constants):\n",
    "    alpha,beta = hyperparameters\n",
    "    num_actions, = constants\n",
    "\n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        # Parameters is the initial perceived reward :\n",
    "        q_initial = jnp.zeros((num_actions,))\n",
    "        \n",
    "        return q_initial # A function of the hyperparameters\n",
    "    \n",
    "    def initial_state(params):\n",
    "        # Initial agent state (beginning of each trial)\n",
    "        \n",
    "        # The initial state is the q_table, as well as an initial action selected (None)\n",
    "        return params,jnp.zeros((num_actions,))\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        gauge_level,reward,trial_over,t = observation\n",
    "        \n",
    "        q_t,previous_action = state\n",
    "        \n",
    "        # Update the table now that we have the new reward !\n",
    "        q_tplus = q_t + alpha*(reward-q_t)*previous_action\n",
    "        \n",
    "        action_distribution = jax.nn.softmax(beta*q_tplus)\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0])       \n",
    "        \n",
    "        return (q_tplus,vect_action_selected),(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        rewards,observations,states,actions = trial_history\n",
    "                \n",
    "        # The params for the next step is the last choice kernel of the trial :\n",
    "        # (the update already occured during the actor step !)\n",
    "        qts,previous_actions = states\n",
    "        \n",
    "        q_t_last = qts[-1]\n",
    "        \n",
    "        \n",
    "        # The params for the next step is the last choice kernel of the trial :\n",
    "        # (the update already occured during the actor step !)\n",
    "        return q_t_last\n",
    "    \n",
    "    \n",
    "    def predict(data_timestep,state,params):\n",
    "        \"\"\"Predict the next action given a set of observations,\n",
    "        as well as the previous internal states and parameters of the agent.\n",
    "\n",
    "        Args:\n",
    "            observation (_type_): _description_\n",
    "            state (_type_): _description_\n",
    "            params (_type_): _description_\n",
    "            true_action : the actual action that was performed (for state updating purposes !)\n",
    "\n",
    "        Returns:\n",
    "            new_state : the \n",
    "            predicted_action : $P(u_t|o_t,s_{t-1},\\theta)$\n",
    "        \"\"\"\n",
    "        gauge_level,obs_bool_filter,reward,true_action,t = data_timestep        \n",
    "        \n",
    "        q_t,previous_action = state\n",
    "        \n",
    "        # Update the table now that we have the new reward !\n",
    "        q_tplus = q_t + alpha*(reward-q_t)*previous_action\n",
    "        \n",
    "        predicted_action = jax.nn.softmax(beta*q_tplus)\n",
    "        \n",
    "        # Here are the data we may want to report during the training : \n",
    "        other_data = None\n",
    "                \n",
    "        return (q_tplus,true_action),predicted_action,other_data\n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    return initial_params,initial_state,actor_step,update_params,predict,None\n",
    "\n",
    "rw_agent_hyperparameters = (0.5,1.0)\n",
    "rw_agent_constants = (9,)\n",
    "final_parameters,predicted_actions,state_history = compute_predicted_actions(synthetic_data,rescorla_wagner_agent(rw_agent_hyperparameters,rw_agent_constants))\n",
    "print(predicted_actions)\n",
    "\n",
    "\n",
    "# We can have an idea of what happened during training by looking at the inner states of the model\n",
    "inner_states,_ = state_history\n",
    "# Of course, these will vary from one model to the next :\n",
    "q_table,previous_action = inner_states\n",
    "print(q_table.shape)\n",
    "print(q_table[-1,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 11, 9)\n",
      "[ 0.          0.11111112  0.11111112 -0.1111111   0.22222221 -0.33333334\n",
      " -0.11111112  0.33333334  0.        ]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def rw_ck_agent(hyperparameters,constants):\n",
    "    alpha,beta,alpha_ck,beta_ck = hyperparameters\n",
    "    num_actions, = constants\n",
    "\n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        # Parameters are the initial perceived reward :\n",
    "        q_initial = jnp.zeros((num_actions,))\n",
    "        # and the initial choice kernel :\n",
    "        ck_initial = jnp.zeros((num_actions,))\n",
    "        \n",
    "        return q_initial,ck_initial \n",
    "    \n",
    "    def initial_state(params):\n",
    "        # Initial agent state (beginning of each trial)\n",
    "        q,ck = params\n",
    "        # The initial state is the q_table, as well as an initial action selected (None)\n",
    "        return q,ck,jnp.zeros((num_actions,))\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        gauge_level,reward,trial_over,t = observation\n",
    "        \n",
    "        q_t,ck,previous_action = state\n",
    "        \n",
    "        # Update the table now that we have the new reward !\n",
    "        q_tplus = q_t + alpha*(reward-q_t)*previous_action\n",
    "        \n",
    "        # Update the choice kernel :\n",
    "        was_a_last_action = jnp.sum(previous_action)  # No update if there was no last action\n",
    "        new_ck = ck + alpha_ck*(previous_action - ck)*was_a_last_action\n",
    "        \n",
    "        \n",
    "        action_distribution = jax.nn.softmax(beta*q_tplus + beta_ck*new_ck)\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0]) \n",
    "        \n",
    "        return (q_tplus,new_ck,vect_action_selected),(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        rewards,observations,states,actions = trial_history\n",
    "        \n",
    "        qts,cks,previous_actions = states\n",
    "        \n",
    "        \n",
    "        q_t_last,ck_last = qts[-1],cks[-1]\n",
    "\n",
    "        return q_t_last,ck_last\n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    def predict(data_timestep,state,params):\n",
    "        \"\"\"Predict the next action given a set of observations,\n",
    "        as well as the previous internal states and parameters of the agent.\n",
    "\n",
    "        Args:\n",
    "            observation (_type_): _description_\n",
    "            state (_type_): _description_\n",
    "            params (_type_): _description_\n",
    "            true_action : the actual action that was performed (for state updating purposes !)\n",
    "\n",
    "        Returns:\n",
    "            new_state : the \n",
    "            predicted_action : $P(u_t|o_t,s_{t-1},\\theta)$\n",
    "        \"\"\"\n",
    "        gauge_level,obs_bool_filter,reward,true_action,t = data_timestep        \n",
    "        \n",
    "        q_t,ck,previous_action = state\n",
    "        \n",
    "        # Update the table now that we have the new reward !\n",
    "        q_tplus = q_t + alpha*(reward-q_t)*previous_action\n",
    "        \n",
    "        # Update the choice kernel :\n",
    "        was_a_last_action = jnp.sum(previous_action)  # No update if there was no last action\n",
    "        new_ck = ck + alpha*(previous_action - ck)*was_a_last_action\n",
    "                \n",
    "        predicted_action = jax.nn.softmax(beta*q_tplus + beta_ck*ck)\n",
    "        \n",
    "        # Here are the data we may want to report during the training : \n",
    "        other_data = None\n",
    "                \n",
    "        return (q_tplus,new_ck,true_action),predicted_action,other_data\n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    return initial_params,initial_state,actor_step,update_params,predict,None\n",
    "\n",
    "\n",
    "rw_ck_agent_hyperparameters = (1.0,1.0,0.5,0.1)\n",
    "rw_ck_agent_constants = (9,)\n",
    "final_parameters,predicted_actions,state_history = compute_predicted_actions(synthetic_data,rw_ck_agent(rw_ck_agent_hyperparameters,rw_ck_agent_constants))\n",
    "\n",
    "# We can have an idea of what happened during training by looking at the inner states of the model\n",
    "inner_states,_ = state_history\n",
    "# Of course, these will vary from one model to the next :\n",
    "q_table,ck_table,previous_action = inner_states\n",
    "print(q_table.shape)\n",
    "print(q_table[-1,-1,:])\n",
    "print(ck_table[-1,-1,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 11, 9, 10)\n",
      "[[ 0.11111111  0.16666667  0.16666669  0.01244444  0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.         -0.09333335  0.03009778 -0.00444444 -0.06222222  0.28444442\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.04444446  0.10000001  0.04000001  0.14222224 -0.07777777  0.\n",
      "   0.          0.05555555  0.          0.        ]\n",
      " [ 0.          0.         -0.07777778 -0.07777778 -0.07777777  0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.12222223  0.03111111 -0.15555556 -0.08888888  0.14222221\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.16666667 -0.00888889 -0.1777778  -0.07777778 -0.23333333  0.\n",
      "   0.         -0.23333333  0.          0.        ]\n",
      " [-0.01111111  0.11111112  0.         -0.0640889  -0.15555556  0.\n",
      "  -0.23333333  0.          0.          0.        ]\n",
      " [ 0.1711111   0.12666667  0.05555556  0.04666667 -0.18666667  0.\n",
      "   0.          0.          0.         -0.15555555]\n",
      " [ 0.          0.         -0.0064      0.12888889 -0.00666667 -0.03422225\n",
      "   0.          0.          0.          0.        ]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def q_learning_agent(hyperparameters,constants):\n",
    "    alpha_plus,alpha_minus,beta,alpha_ck,beta_ck = hyperparameters\n",
    "    num_actions,num_states = constants\n",
    "\n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        # Parameters are the initial q-table. As opposed to a RW agent, the mappings now depend on the states \n",
    "        # This usually allows for better responsiveness to the environment, but in this situation it may make the training\n",
    "        # harder !\n",
    "        q_initial = jnp.zeros((num_actions,num_states))\n",
    "        # and the initial choice kernel :\n",
    "        ck_initial = jnp.zeros((num_actions,))\n",
    "        \n",
    "        return q_initial,ck_initial \n",
    "    \n",
    "    def initial_state(params):\n",
    "        # Initial agent state (beginning of each trial)\n",
    "        q,ck = params\n",
    "        # The initial state is the q_table, as well as an initial action selected (None) and the last gauge level (None)\n",
    "        return q,ck,jnp.zeros((num_actions,)),[jnp.zeros((num_states,))]\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        current_stimuli,reward,trial_over,t = observation\n",
    "        current_gauge_level = current_stimuli[0]\n",
    "        \n",
    "        q_t,ck,previous_action,previous_stimuli = state\n",
    "        previous_gauge_level = previous_stimuli[0]\n",
    "        \n",
    "        # Update the table now that we have the new reward !\n",
    "        # This is \"where\" the reward was observed in the table :\n",
    "        previous_action_state = jnp.einsum(\"i,j->ij\",previous_action,previous_gauge_level)\n",
    "        \n",
    "        positive_reward = jnp.clip(reward,min=0.0)\n",
    "        negative_reward = jnp.clip(reward,max=0.0)\n",
    "        \n",
    "        positive_reward_prediction_error = positive_reward - q_t\n",
    "        negative_reward_prediction_error = negative_reward - q_t\n",
    "        \n",
    "        q_tplus = q_t + (alpha_plus*positive_reward_prediction_error + alpha_minus*negative_reward_prediction_error)*previous_action_state\n",
    "        \n",
    "        # Update the choice kernel :\n",
    "        was_a_last_action = jnp.sum(previous_action)  # No update if there was no last action\n",
    "        new_ck = ck + alpha_ck*(previous_action - ck)*was_a_last_action\n",
    "        \n",
    "\n",
    "\n",
    "        # Action selection :\n",
    "        q_table_at_this_state = jnp.einsum(\"ij,j->i\",q_tplus,current_gauge_level)\n",
    "        \n",
    "        action_distribution = jax.nn.softmax(beta*q_table_at_this_state + beta_ck*new_ck)\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0])  \n",
    "        \n",
    "        return (q_tplus,new_ck,vect_action_selected,current_stimuli),(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        rewards,observations,states,actions = trial_history\n",
    "        \n",
    "        qts,cks,previous_actions,previous_stimuli = states\n",
    "        \n",
    "        q_t_last,ck_last = qts[-1],cks[-1]\n",
    "        \n",
    "        # The params for the next step is the last choice kernel of the trial :\n",
    "        # (the update already occured during the actor step !)\n",
    "        return q_t_last,ck_last\n",
    "    \n",
    "    def predict(data_timestep,state,params):\n",
    "        \"\"\"Predict the next action given a set of observations,\n",
    "        as well as the previous internal states and parameters of the agent.\n",
    "\n",
    "        Args:\n",
    "            observation (_type_): _description_\n",
    "            state (_type_): _description_\n",
    "            params (_type_): _description_\n",
    "            true_action : the actual action that was performed (for state updating purposes !)\n",
    "\n",
    "        Returns:\n",
    "            new_state : the \n",
    "            predicted_action : $P(u_t|o_t,s_{t-1},\\theta)$\n",
    "        \"\"\"\n",
    "        current_stimuli,obs_bool_filter,reward,true_action,t = data_timestep      \n",
    "        current_gauge_level = current_stimuli[0]  \n",
    "        \n",
    "        q_t,ck,previous_action,previous_stimuli = state\n",
    "        previous_gauge_level = previous_stimuli[0]\n",
    "        \n",
    "        # Update the table now that we have the new reward !\n",
    "        # This is \"where\" the reward was observed in the table : \n",
    "        previous_action_state = jnp.einsum(\"i,j->ij\",previous_action,previous_gauge_level)\n",
    "        \n",
    "        positive_reward = jnp.clip(reward,min=0.0)\n",
    "        negative_reward = jnp.clip(reward,max=0.0)\n",
    "        \n",
    "        positive_reward_prediction_error = positive_reward - q_t\n",
    "        negative_reward_prediction_error = negative_reward - q_t\n",
    "        \n",
    "        q_tplus = q_t + (alpha_plus*positive_reward_prediction_error + alpha_minus*negative_reward_prediction_error)*previous_action_state\n",
    "        \n",
    "        # Update the choice kernel :\n",
    "        was_a_last_action = jnp.sum(previous_action)  # No update if there was no last action\n",
    "        new_ck = ck + alpha_ck*(previous_action - ck)*was_a_last_action\n",
    "        \n",
    "\n",
    "        # Action selection :\n",
    "        q_table_at_this_state = jnp.einsum(\"ij,j->i\",q_tplus,current_gauge_level)\n",
    "        \n",
    "        predicted_action = jax.nn.softmax(beta*q_table_at_this_state + beta_ck*new_ck)\n",
    "        \n",
    "        # Here are the data we may want to report during the training : \n",
    "        other_data = None\n",
    "        \n",
    "        return (q_tplus,new_ck,true_action,current_stimuli),predicted_action,other_data\n",
    "            # ____________________________________________________________________________________________\n",
    "    \n",
    "    return initial_params,initial_state,actor_step,update_params,predict,None\n",
    "\n",
    "\n",
    "ql_ck_agent_hyperparameters = (0.5,0.7,1.0,0.0,0.0)\n",
    "ql_ck_agent_constants = (9,N_FEEDBACK_OUTCOMES)\n",
    "final_parameters,predicted_actions,state_history = compute_predicted_actions(synthetic_data,q_learning_agent(ql_ck_agent_hyperparameters,ql_ck_agent_constants))\n",
    "\n",
    "# We can have an idea of what happened during training by looking at the inner states of the model\n",
    "inner_states,_ = state_history\n",
    "# Of course, these will vary from one model to the next :\n",
    "q_table,ck_table,previous_action,previous_stim = inner_states\n",
    "print(q_table.shape)\n",
    "print(q_table[-1,-1,:])\n",
    "print(ck_table[-1,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(10, 5)\n",
      "(10,)\n",
      "(10, 5)\n",
      "(11, 5)\n",
      "[[[-2.4443073]\n",
      "  [-2.1292756]\n",
      "  [-2.5646544]\n",
      "  [-2.3293624]\n",
      "  [-2.1623502]\n",
      "  [-2.2017615]\n",
      "  [-2.5376027]\n",
      "  [-2.3271432]\n",
      "  [-2.3047287]\n",
      "  [-2.570928 ]\n",
      "  [-2.382975 ]]\n",
      "\n",
      " [[-2.3424745]\n",
      "  [-2.3354323]\n",
      "  [-2.2052903]\n",
      "  [-2.1768618]\n",
      "  [-1.9983485]\n",
      "  [-1.7983079]\n",
      "  [-2.4819963]\n",
      "  [-2.197515 ]\n",
      "  [-2.1801658]\n",
      "  [-2.1749773]\n",
      "  [-2.4451654]]\n",
      "\n",
      " [[-2.4443073]\n",
      "  [-2.2808294]\n",
      "  [-2.3455386]\n",
      "  [-2.2459264]\n",
      "  [-2.1588576]\n",
      "  [-2.0376785]\n",
      "  [-2.1240754]\n",
      "  [-2.307764 ]\n",
      "  [-2.2648354]\n",
      "  [-2.5073297]\n",
      "  [-2.4114966]]\n",
      "\n",
      " [[-2.4802887]\n",
      "  [-2.3189125]\n",
      "  [-2.351154 ]\n",
      "  [-2.31735  ]\n",
      "  [-1.8719108]\n",
      "  [-1.8780396]\n",
      "  [-2.426797 ]\n",
      "  [-2.2695749]\n",
      "  [-2.093858 ]\n",
      "  [-2.4992175]\n",
      "  [-2.3067732]]\n",
      "\n",
      " [[-2.1938996]\n",
      "  [-2.3390865]\n",
      "  [-2.2500815]\n",
      "  [-2.1948395]\n",
      "  [-2.1000633]\n",
      "  [-2.1135259]\n",
      "  [-2.5023055]\n",
      "  [-2.3099043]\n",
      "  [-2.2700286]\n",
      "  [-2.5038948]\n",
      "  [-2.3152785]]\n",
      "\n",
      " [[-2.3424745]\n",
      "  [-2.4746828]\n",
      "  [-2.3994393]\n",
      "  [-2.343054 ]\n",
      "  [-2.3191104]\n",
      "  [-2.239571 ]\n",
      "  [-2.2650075]\n",
      "  [-2.1434603]\n",
      "  [-2.0928001]\n",
      "  [-1.9235013]\n",
      "  [-1.9251459]]\n",
      "\n",
      " [[-2.1043572]\n",
      "  [-2.1323395]\n",
      "  [-1.8655345]\n",
      "  [-2.12423  ]\n",
      "  [-1.90568  ]\n",
      "  [-2.4616814]\n",
      "  [-2.1358802]\n",
      "  [-2.1045134]\n",
      "  [-2.082408 ]\n",
      "  [-1.9858627]\n",
      "  [-1.6552625]]\n",
      "\n",
      " [[-2.4802887]\n",
      "  [-2.2670872]\n",
      "  [-2.2532916]\n",
      "  [-2.31795  ]\n",
      "  [-2.070984 ]\n",
      "  [-2.154631 ]\n",
      "  [-2.049962 ]\n",
      "  [-1.8937852]\n",
      "  [-2.317112 ]\n",
      "  [-2.1105857]\n",
      "  [-2.056078 ]]\n",
      "\n",
      " [[-2.1938996]\n",
      "  [-1.814564 ]\n",
      "  [-1.5974976]\n",
      "  [-2.4103043]\n",
      "  [-2.2561762]\n",
      "  [-2.1002762]\n",
      "  [-2.3524852]\n",
      "  [-2.2903523]\n",
      "  [-2.2902105]\n",
      "  [-2.3204565]\n",
      "  [-2.4801455]]\n",
      "\n",
      " [[-2.3424745]\n",
      "  [-2.2271867]\n",
      "  [-2.1663008]\n",
      "  [-2.185037 ]\n",
      "  [-2.2069907]\n",
      "  [-2.3618765]\n",
      "  [-2.1874614]\n",
      "  [-2.165276 ]\n",
      "  [-2.1653557]\n",
      "  [-2.1919541]\n",
      "  [-2.1109152]]]\n",
      "(10, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def active_inference_basic_1D(hyperparameters,constants):\n",
    "    a0,b0,c0,d0,e0,u = basic_latent_model({**constants, **hyperparameters})\n",
    "    beta = hyperparameters[\"action_selection_temperature\"]\n",
    "    \n",
    "    planning_options = get_planning_options(constants[\"Th\"],\"classic\",a_novel=False,b_novel=False)\n",
    "    learning_options = get_learning_options(learn_b=True,lr_b=hyperparameters[\"transition_learning_rate\"],method=\"vanilla+backwards\",\n",
    "                                    state_generalize_function=lambda x : jnp.exp(-hyperparameters[\"state_interpolation_temperature\"]),\n",
    "                                    action_generalize_table=None,\n",
    "                                    cross_action_extrapolation_coeff=None)\n",
    "    # ____________________________________________________________________________________________\n",
    "    # Each agent is a set of functions of the form :    \n",
    "    def initial_params():\n",
    "        # The initial parameters of the AIF agent are its model weights :\n",
    "        return a0,b0,c0,d0,e0,u\n",
    "    \n",
    "    def initial_state(params):\n",
    "        pa,pb,pc,pd,pe,u = params\n",
    "\n",
    "        # The \"states\" of the active Inference agent are : \n",
    "        # 1. The vectorized parameters for this trial :\n",
    "        trial_a,trial_b,trial_d = vectorize_weights(pa,pb,pd,u)\n",
    "        trial_c,trial_e = to_log_space(pc,pe)\n",
    "        trial_a_nov,trial_b_nov = get_vectorized_novelty(pa,pb,u,compute_a_novelty=True,compute_b_novelty=True)\n",
    "        \n",
    "        # 2. Its priors about the next state : (given by the d matrix parameter)\n",
    "        prior = trial_d\n",
    "        \n",
    "        return prior,jnp.zeros_like(prior),(trial_a,trial_b,trial_c,trial_e,trial_a_nov,trial_b_nov) # We don't need trial_d anymore !\n",
    "\n",
    "    def actor_step(observation,state,params,rng_key):\n",
    "        emission,reward,trial_over,t = observation\n",
    "        gauge_level = emission[0]\n",
    "                \n",
    "        state_prior,previous_posterior,timestep_weights = state\n",
    "        a_norm,b_norm,c,e,a_novel,b_novel = timestep_weights\n",
    "        \n",
    "        end_of_trial_filter = jnp.ones((planning_options[\"horizon\"]+2,))\n",
    "        qs,F,raw_qpi,efe = compute_step_posteriors(t,state_prior,emission,a_norm,b_norm,c,e,a_novel,b_novel,\n",
    "                                    end_of_trial_filter,\n",
    "                                    rng_key,planning_options)       \n",
    "\n",
    "        # Action selection :        \n",
    "        action_distribution = jax.nn.softmax(beta*efe)\n",
    "        action_selected = jr.categorical(rng_key,_jaxlog(action_distribution))\n",
    "        vect_action_selected = jax.nn.one_hot(action_selected,action_distribution.shape[0])  \n",
    "        \n",
    "        # New state prior : \n",
    "        new_prior = jnp.einsum(\"iju,j,u->i\",b_norm,qs,vect_action_selected)\n",
    "        \n",
    "        # OPTIONAL : ONLINE UPDATING OF PARAMETERS \n",
    "        \n",
    "        return (new_prior,timestep_weights),(action_distribution,action_selected,vect_action_selected)\n",
    "\n",
    "\n",
    "    def update_params(trial_history,params):\n",
    "        pa,pb,pc,pd,pe,u = params\n",
    "        rewards,observations,states,actions = trial_history\n",
    "        \n",
    "        priors_history,posteriors_history,_ = states   \n",
    "\n",
    "        obs_vect_arr = [jnp.array(observations[0])]\n",
    "        qs_arr = jnp.stack(posteriors_history)\n",
    "        u_vect_arr = jnp.stack(actions)    \n",
    "        \n",
    "        # Then, we update the parameters of our HMM model at this level\n",
    "        # We use the raw weights here !\n",
    "        a_post,b_post,c_post,d_post,e_post,qs_post = learn_after_trial(obs_vect_arr,qs_arr,u_vect_arr,\n",
    "                                                pa,pb,c,pd,e,u,\n",
    "                                                method = learning_options[\"method\"],\n",
    "                                                learn_what = learning_options[\"bool\"],\n",
    "                                                learn_rates = learning_options[\"rates\"],\n",
    "                                                generalize_state_function=learning_options[\"state_generalize_function\"],\n",
    "                                                generalize_action_table=learning_options[\"action_generalize_table\"],\n",
    "                                                cross_action_extrapolation_coeff=learning_options[\"cross_action_extrapolation_coeff\"],\n",
    "                                                em_iter = learning_options[\"em_iterations\"])\n",
    "        \n",
    "        # The params for the next step is the last choice kernel of the trial :\n",
    "        # (the update already occured during the actor step !)\n",
    "        return a_post,b_post,c_post,d_post,e_post,u\n",
    "    # ____________________________________________________________________________________________\n",
    "    \n",
    "    def predict(data_timestep,state,params):\n",
    "        \"\"\"Predict the next action given a set of observations,\n",
    "        as well as the previous internal states and parameters of the agent.\n",
    "\n",
    "        Args:\n",
    "            observation (_type_): _description_\n",
    "            state (_type_): _description_\n",
    "            params (_type_): _description_\n",
    "            true_action : the actual action that was performed (for state updating purposes !)\n",
    "\n",
    "        Returns:\n",
    "            new_state : the \n",
    "            predicted_action : $P(u_t|o_t,s_{t-1},\\theta)$\n",
    "        \"\"\"\n",
    "        current_stimuli,obs_bool_filter,reward,true_action,t = data_timestep      \n",
    "        current_gauge_level = current_stimuli[0]  \n",
    "                \n",
    "        state_prior,previous_posterior,timestep_weights = state\n",
    "        a_norm,b_norm,c,e,a_novel,b_novel = timestep_weights\n",
    "        \n",
    "        end_of_trial_filter = jnp.ones((planning_options[\"horizon\"]+2,))\n",
    "        qs,F,raw_qpi,efe = compute_step_posteriors(t,state_prior,current_stimuli,a_norm,b_norm,c,e,a_novel,b_novel,\n",
    "                                    end_of_trial_filter,\n",
    "                                    None,planning_options)       \n",
    "\n",
    "        # Action selection :        \n",
    "        predicted_action = jax.nn.softmax(beta*efe)\n",
    "        \n",
    "        # New state prior : \n",
    "        new_prior = jnp.einsum(\"iju,j,u->i\",b_norm,qs,true_action)\n",
    "        \n",
    "        # OPTIONAL : ONLINE UPDATING OF PARAMETERS \n",
    "                \n",
    "        # Here are the data we may want to report during the training : \n",
    "        other_data = (qs,F)\n",
    "        \n",
    "        return (new_prior,qs,timestep_weights),predicted_action,other_data\n",
    "        # ____________________________________________________________________________________________         \n",
    "    return initial_params,initial_state,actor_step,update_params,predict,None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We get a model weights by defining a \"parameters\" object :\n",
    "aif_1d_constants = {\n",
    "    # General environment : \n",
    "    \"N_feedback_ticks\":N_FEEDBACK_OUTCOMES,\n",
    "    # Latent state space structure\n",
    "    \"Ns_latent\":5,      # For 1D\n",
    "    # Action discretization:\n",
    "    \"N_actions_distance\" :3,\n",
    "    \"N_actions_position\" :9,\n",
    "    \"N_actions_angle\" :9,\n",
    "    \n",
    "    \"Th\" : 3\n",
    "}\n",
    "\n",
    "aif_1d_params = {    \n",
    "    # ----------------------------------------------------------------------------------------------------\n",
    "    # Model parameters : these should interact with the model components in a differentiable manner\n",
    "    \"transition_concentration\": 1.0,\n",
    "    \"transition_stickiness\": 1.0,\n",
    "    \"transition_learning_rate\" : 1.0,\n",
    "    \"state_interpolation_temperature\" : 1.0,\n",
    "    \n",
    "    \"initial_state_concentration\": 1.0,\n",
    "    \n",
    "    \"feedback_expected_std\" : 0.15,\n",
    "    \"emission_concentration\" : 1.0,\n",
    "    \"emission_stickiness\" : 100.0,\n",
    "    \n",
    "    \"reward_seeking\" : 10.0,\n",
    "    \n",
    "    \"action_selection_temperature\" : 1.0,\n",
    "}\n",
    "final_parameters,predicted_actions,state_history = compute_predicted_actions(synthetic_data,active_inference_basic_1D(aif_1d_params,aif_1d_constants))\n",
    "# print(predicted_actions)\n",
    "\n",
    "\n",
    "# We can have an idea of what happened during training by looking at the inner states of the model\n",
    "inner_states,(infered_states,free_energies) = state_history\n",
    "# Of course, these will vary from one model to the next :\n",
    "priors,posteriors,weights = inner_states\n",
    "\n",
    "# a_norm,b_norm,c,e,a_novel,b_novel = weights\n",
    "\n",
    "print(free_energies)\n",
    "print(free_energies.shape)\n",
    "\n",
    "# print(q_table.shape)\n",
    "# print(q_table[-1,-1,:])\n",
    "# print(ck_table[-1,-1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of the study, we'll stock all of these predictive models in the [proposal models](./proposal_models.py) file to manipulate and change them more easily. Next up is inversion !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploit_results_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
