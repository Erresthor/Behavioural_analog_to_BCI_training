{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational modeling : RL algorithms in a virtual environment\n",
    "\n",
    "Question : under very low amounts of evidence, how do human sample a complex action space ? Can we infer some form of structure in this exploration ? Can Active Inference provide some answers regarding the mechanistic processes behind it ?\n",
    "\n",
    "\n",
    "\n",
    "First, we grab the data corresponding to the experiment we're interested in (here, experiment 002). We also remove the subjects that either had technical issues or had very suspicious results. *(we should provide a clear rule on subject exclusion here, maybe based on action variance across all dimensions or reaction times ?).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the task results for 90 subjects.\n",
      "89 subjects remaining after removing problematic subjects.\n"
     ]
    }
   ],
   "source": [
    "# Import the needed packages \n",
    "# \n",
    "# 1/ the usual suspects\n",
    "import sys,os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# 2/ The Active Inference package \n",
    "import actynf\n",
    "\n",
    "# 3/ Tools for : \n",
    "# a. Getting the raw data : \n",
    "from database_handling.database_extract import get_all_subject_data_from_internal_task_id\n",
    "from utils import remove_by_indices\n",
    "# b. Preprocessing the data :\n",
    "from analysis_tools.preprocess import OPTIONS_PREPROCESS_DEFAULT,get_preprocessed_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Except subjects for predictors :\n",
    "problematic_subjects_misc = [\"5c9cb670b472d0001295f377\"]\n",
    "        # This subject has read the instructions with one submission and ran\n",
    "        # the actual task with another, rendering statistics computed impossible to \n",
    "        # compare, this should be substracted from any statistical models based on\n",
    "        # instructional data, but can be kept for raw performance plots.\n",
    "# problematic_subjects_fraudulent =[\"6595ae358923ce48b037a0dc\"]\n",
    "        # This subject has very suspicious responses, including always putting both points in the same place\n",
    "        # and acting as quickly as possible, to be removed from all analysis ?\n",
    "\n",
    "\n",
    "# Import the data from the remote mongodb database & the imported prolific demographics :\n",
    "INTERNAL_TASK_ID = \"002\"\n",
    "TASK_RESULTS_ALL = get_all_subject_data_from_internal_task_id(INTERNAL_TASK_ID,None,\n",
    "                                        autosave=True,override_save=False,autoload=True)\n",
    "print(\"Loaded the task results for \" + str(len(TASK_RESULTS_ALL)) + \" subjects.\")\n",
    "\n",
    "# Each subject in task results has the following entries : \n",
    "# TASK_RESULT_FEATURES, TASK_RESULTS_EVENTS, TASK_RESULTS_DATA, TASK_RESULTS,RT_FB\n",
    "remove_these_subjects = []\n",
    "for index,entry in enumerate(TASK_RESULTS_ALL):\n",
    "    subj_dict,_,_,_ = entry\n",
    "    subj_name = subj_dict[\"subject_id\"]\n",
    "    if subj_name in problematic_subjects_misc:\n",
    "        remove_these_subjects.append(index)\n",
    "\n",
    "TASK_RESULTS = remove_by_indices(TASK_RESULTS_ALL,remove_these_subjects)\n",
    "print(str(len(TASK_RESULTS)) + \" subjects remaining after removing problematic subjects.\")\n",
    "\n",
    "\n",
    "LABELS = [entry[0] for entry in TASK_RESULTS]\n",
    "EVENTS = [entry[1] for entry in TASK_RESULTS]\n",
    "TRIAL_DATA = [entry[2] for entry in TASK_RESULTS]\n",
    "RT_FBS = [entry[3] for entry in TASK_RESULTS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the \"raw data\" is loaded, we can use the preprocessing pipeline described briefly [here](./computational_modeling_101_preprocessing.ipynb) to generate a dictionnary with the observations and actions ready for use in our models. \n",
    "\n",
    "We can generate several dictionnaries depending on how we want the data to be formatted. This is driven by the option dictionnary :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of the 9790.0 actions performed by our subjects, 8233.0 were 'valid' (84.1 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annic\\OneDrive\\Bureau\\MainPhD\\code\\behavioural_exp_code\\exploit_results_env\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\annic\\OneDrive\\Bureau\\MainPhD\\code\\behavioural_exp_code\\exploit_results_env\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of the 9790.0 feedback sequences potentially observed by our subjects, 8803 were 'valid' (89.9 %)\n",
      "dict_keys(['observations', 'actions'])\n"
     ]
    }
   ],
   "source": [
    "preprocessing_options = {\n",
    "    \"actions\":{\n",
    "        \"distance_bins\" : np.array([0.0,0.05,0.2,0.5,jnp.sqrt(2) + 1e-10]),\n",
    "        \"angle_N_bins\"  : 8,\n",
    "        \"position_N_bins_per_dim\" : 3\n",
    "    },\n",
    "    \"observations\":{\n",
    "        \"N_bins\" : 10,\n",
    "        \"observation_ends_at_point\" : 2\n",
    "    }\n",
    "}\n",
    "    # We can modify these at will\n",
    "\n",
    "data = get_preprocessed_data(TRIAL_DATA,RT_FBS,preprocessing_options,\n",
    "                            verbose=True,\n",
    "                            autosave=True,autoload=True,override_save=True,\n",
    "                            label=\"default\")\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to the models ! \n",
    "# Let us grab a model of the training environment itself : \n",
    "\n",
    "# ENVIRONMENTAL CONSTANTS :\n",
    "GRID_SIZE = (7,7)\n",
    "START_COORD = [[5,1],[5,2],[4,1]]\n",
    "END_COORD = [0,6]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We define the environment as a state machine that outputs a feedback \n",
    "# every time an action is given to it : \n",
    "from actynf.jaxtynf.layer_process import initial_state_and_obs,process_update\n",
    "from actynf.jaxtynf.shape_tools import vectorize_weights\n",
    "\n",
    "class TrainingEnvironment :\n",
    "    def __init__(self,rng_key,a,b,c,d,e,u,T):\n",
    "        # Environment parameters\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c \n",
    "        self.d = d\n",
    "        self.e = e\n",
    "        self.u = u\n",
    "        \n",
    "        # Timing\n",
    "        self.Ntimesteps = T\n",
    "        self.t = 0\n",
    "        self.rng_key = rng_key\n",
    "        \n",
    "        # Inner state\n",
    "        self.current_state = None\n",
    "        \n",
    "        self.update_vectorized_weights()\n",
    "    \n",
    "    def update_vectorized_weights(self):\n",
    "        self.vec_a,self.vec_b,self.vec_d = vectorize_weights(self.a,self.b,self.d,u)\n",
    "    \n",
    "    def reinit_trial(self):\n",
    "        self.t = 0\n",
    "        \n",
    "        self.rng_key,init_tmstp_key = jax.random.split(self.rng_key)\n",
    "        [s_d,s_idx,s_vect],[o_d,o_idx,o_vect] = initial_state_and_obs(init_tmstp_key,self.vec_a,self.vec_d)\n",
    "        \n",
    "        self.current_state = s_vect\n",
    "        \n",
    "        return o_vect,True        \n",
    "    \n",
    "    def step(self,action_chosen):\n",
    "        \n",
    "        if self.t == self.Ntimesteps:\n",
    "            print(\"New trial ! The action has not been used here.\")\n",
    "            return self.reinit_trial()\n",
    "        \n",
    "        self.rng_key,timestep_rngkey = jax.random.split(self.rng_key)\n",
    "        [s_d,s_idx,s_vect],[o_d,o_idx,o_vect] = process_update(timestep_rngkey,self.current_state,self.vec_a,self.vec_b,action_chosen)\n",
    "        \n",
    "        self.t = self.t + 1   \n",
    "         \n",
    "        return o_vect,False\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.TrainingEnvironment object at 0x00000213F2039B40>\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The environment is statically defined by its HMM matrices : \n",
    "from models import behavioural_process\n",
    "\n",
    "T = 10\n",
    "(a,b,c,d,e,u),fb_vals = behavioural_process(GRID_SIZE,START_COORD,END_COORD,preprocessing_options[\"observations\"][\"N_bins\"],0.15)\n",
    "\n",
    "rngkey = jax.random.PRNGKey(0)\n",
    "env = TrainingEnvironment(rngkey,a,b,c,d,e,u,T)\n",
    "print(env)\n",
    "\n",
    "for k in range(100):\n",
    "    o,new_trial = env.reinit_trial()\n",
    "    print(o)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now interested in how multiple kinds of complex RL agents may perform in this situation. (this may depend on hyperparamters ...) \n",
    "\n",
    "Here is a repository with a lot of candidate algorithms :\n",
    "https://github.com/udacity/deep-reinforcement-learning/blob/master/cross-entropy/CEM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4 on Reinforcement learning algorithms without Neural Networks. Because our system is quite simple, we don't necessarily need to use amortization components and may focus on more traditionnal (and easy to fit) methods !\n",
    "\n",
    "### 1. **Tabular Methods**\n",
    "   These methods explicitly store and update values for each state or state-action pair in a table. They are suitable for environments with a small number of states and actions.\n",
    "\n",
    "   - **Q-learning (Tabular)**: \n",
    "     - A widely used off-policy method where a Q-value table is maintained, and values are updated based on the Bellman equation.\n",
    "     - No neural network is involved; the Q-values are stored explicitly in a table.\n",
    "   \n",
    "   - **SARSA (Tabular)**: \n",
    "     - Similar to Q-learning but it is an on-policy method, updating the action-value function based on the action chosen by the current policy.\n",
    "\n",
    "   - **Monte Carlo Methods**: \n",
    "     - These methods estimate the value of a policy by averaging returns following episodes of interaction with the environment.\n",
    "     - For every state (or state-action pair), returns are recorded and averaged to compute values.\n",
    "\n",
    "   - **Dynamic Programming (DP)**: \n",
    "     - Methods like **policy iteration** and **value iteration** use a model of the environment's dynamics to compute optimal policies.\n",
    "     - These rely on a table to store value functions or policies.\n",
    "\n",
    "### 2. **Function Approximation (without Neural Networks)**\n",
    "   When state spaces are too large for tabular methods, simpler function approximation techniques are used to represent value functions or policies.\n",
    "\n",
    "   - **Linear Function Approximation**: \n",
    "     - The value of a state (or state-action pair) is approximated as a linear combination of features.\n",
    "     - For example, a value function can be represented as $ V(s) = w^T \\phi(s) $, where $ w $ is a vector of weights and $ \\phi(s) $ is a feature vector of the state.\n",
    "\n",
    "   - **Tile Coding**: \n",
    "     - A method for discretizing a continuous state space into tiles or grids, where values are maintained for each tile.\n",
    "     - It is often used with linear approximations to represent values for continuous environments.\n",
    "\n",
    "   - **Polynomial or Radial Basis Function (RBF) Approximations**: \n",
    "     - These methods represent the value function using predefined basis functions, such as polynomials or radial basis functions.\n",
    "\n",
    "### 3. **Policy Gradient Methods without Neural Networks**\n",
    "   Policy gradient methods can be implemented without neural networks by using simpler parameterizations of the policy.\n",
    "\n",
    "   - **Linear Policies**: \n",
    "     - The policy is represented as a linear combination of features, and the parameters of the linear policy are optimized using policy gradient techniques.\n",
    "\n",
    "   - **Softmax Policies with Tabular Representation**: \n",
    "     - For discrete action spaces, a policy can be represented as a probability distribution (e.g., softmax) over actions for each state, and these probabilities are updated directly.\n",
    "\n",
    "### 4. **Model-Based Methods**\n",
    "   These methods involve learning or using a model of the environment's dynamics to help plan and optimize policies.\n",
    "\n",
    "   - **Value Iteration (with Model)**: \n",
    "     - Given a model of the environment (a transition function and reward function), value iteration updates the value of each state using Bellman's equation.\n",
    "   \n",
    "   - **Policy Iteration (with Model)**: \n",
    "     - Alternates between policy evaluation and policy improvement to converge on the optimal policy using the model of the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploit_results_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
