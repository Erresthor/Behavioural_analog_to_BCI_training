{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A behavioural analog to BCI training : results\n",
    "\n",
    "Statistics and simple plots of the results of the experiment (technical test). \n",
    "\n",
    "This notebook does not figure much computational modeling (yet).\n",
    "\n",
    "\n",
    "Let's start by importing the necessary packages as well as the data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Loaded the task results for study 6703ab18d345eaa4893587e0 \n",
      "    (49 subjects.)\n",
      " - Loaded the task results for study 66f9aee8210357265a5958fc \n",
      "    (50 subjects.)\n",
      " - Loaded the task results for study 6703ab1a7ea30557549dc6da \n",
      "    (50 subjects.)\n",
      "Total : 149 subjects\n",
      "145 subjects remaining after removing problematic subjects.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import plotly as pltly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from jax import vmap\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# + local functions : \n",
    "from database_handling.database_extract import get_all_subject_data_from_internal_task_id\n",
    "from utils import remove_by_indices\n",
    "\n",
    "# Except subjects for predictors :\n",
    "problematic_subjects_misc = [\"611d60c383f4f70ff4bc99fd\", # S2 : Did the task twice \n",
    "                             \"66a74bdfdcaccdc0703894d5\", # Consent revoked \n",
    "                            \"667d92f2ea5c1542f417285d\",\n",
    "                            \"6548f570022275786186ffbd\"]\n",
    "                \n",
    "                \n",
    "\n",
    "# problematic_subjects_fraudulent =[\"66bb5c09526e6d80f1146800\"]\n",
    "                # This subject had \"eclectic performances\" so say the least\n",
    "                # Left fullscreen twice and missed 4 actions\n",
    "\n",
    "\n",
    "# Import the data from the remote mongodb database & the imported prolific demographics :\n",
    "INTERNAL_TASK_ID = \"003\"\n",
    "# Study 2 : the std of the was 0.025,0.1 and 0.175\n",
    "PROLIFIC_STUDY_IDs = [\"6703ab18d345eaa4893587e0\",\"66f9aee8210357265a5958fc\",\"6703ab1a7ea30557549dc6da\"]\n",
    "\n",
    "TASK_RESULTS_ALL = []\n",
    "for prolific_study_id in PROLIFIC_STUDY_IDs:\n",
    "    task_results = get_all_subject_data_from_internal_task_id(INTERNAL_TASK_ID,prolific_study_id,\n",
    "                                                              process_feedback_data_stream=True,override_save=False)\n",
    "    print(\" - Loaded the task results for study {} \\n    ({} subjects.)\".format(prolific_study_id,len(task_results)))\n",
    "    TASK_RESULTS_ALL += task_results\n",
    "print(\"Total : {} subjects\".format(len(TASK_RESULTS_ALL)))\n",
    "\n",
    "# Each subject in task results has the following entries : \n",
    "# TASK_RESULT_FEATURES, TASK_RESULTS_EVENTS, TASK_RESULTS_DATA, TASK_RESULTS,RT_FB\n",
    "remove_these_subjects = []\n",
    "for index,entry in enumerate(TASK_RESULTS_ALL):\n",
    "    subj_dict,_,_,_ = entry\n",
    "    subj_name = subj_dict[\"subject_id\"]\n",
    "    if subj_name in problematic_subjects_misc:\n",
    "        remove_these_subjects.append(index)\n",
    "\n",
    "TASK_RESULTS = remove_by_indices(TASK_RESULTS_ALL,remove_these_subjects)\n",
    "print(str(len(TASK_RESULTS)) + \" subjects remaining after removing problematic subjects.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataframe building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 11, 10, 2, 4)\n",
      "A total of 33/15950 actions were missed. (0.21 %)\n"
     ]
    }
   ],
   "source": [
    "# The initial datframe is the first tuple in our task result list of tuples : \n",
    "subjects_df = pd.DataFrame([entry[0] for entry in TASK_RESULTS])\n",
    "\n",
    "# Avoid too many categories : \n",
    "subjects_df['Sex'] = np.where(subjects_df['Sex'].isin(['Male','Female']), subjects_df['Sex'], 'Other')\n",
    "\n",
    "category_counts = subjects_df['Nationality'].value_counts()\n",
    "threshold = 2\n",
    "subjects_df['Nationality_red'] = subjects_df['Nationality'].apply(lambda x: x if category_counts[x] >= threshold else 'Other')\n",
    "\n",
    "# There was a single noise term for the whole training for each subject : \n",
    "subject_noise_parameters = [np.array(entry[2][\"parameters\"][\"noise_int\"])[0] for entry in TASK_RESULTS]\n",
    "\n",
    "# We add it to the df : \n",
    "subjects_df[\"feedback_noise_std\"] = subject_noise_parameters\n",
    "\n",
    "# # Create a pandas dataframe from the list of subject dictionnaries :\n",
    "\n",
    "# In this dataframe, we're interested in sotring various kinds of data from the trials : \n",
    "# 1/ Data from the instruction phase\n",
    "\n",
    "# 2/ Data from the feedback gauge :\n",
    "# Timestep values :\n",
    "all_subject_scores = [subjdata[2][\"scoring\"] for subjdata in TASK_RESULTS]\n",
    "subjects_df[\"raw_feedback_values\"] = [subj_scores[\"feedback\"] for subj_scores in all_subject_scores]\n",
    "# Real time gauge values :\n",
    "subjects_df[\"realtime_values\"] = [subjdata[3][1] for subjdata in TASK_RESULTS] # Each element is a list of list os arrays (with varying shape)\n",
    "\n",
    "# 3/ Data from the hidden grid :\n",
    "# The grid for a specific trial: \n",
    "trial_grids = [entry[2][\"process\"][\"grids\"] for entry in TASK_RESULTS]\n",
    "subjects_df[\"grid_layout\"] = trial_grids\n",
    "# Position value :\n",
    "subject_positions = [entry[2][\"process\"][\"positions\"] for entry in TASK_RESULTS]\n",
    "goal_positions = [np.array(entry[2][\"parameters\"][\"goal_pos\"])[:,0,:] for entry in TASK_RESULTS]\n",
    "subjects_df[\"grid_position\"] = goal_positions\n",
    "\n",
    "def euclidian_distance(position,goal):\n",
    "    return jnp.linalg.norm(position-goal,2)\n",
    "gs = trial_grids[0][0].shape\n",
    "maximum_euclidian_dist = euclidian_distance(jnp.array(gs) - jnp.ones((2,)),jnp.zeros((2,)))\n",
    "all_euclidian_distances = vmap(vmap(vmap(euclidian_distance,in_axes=(0,None))))(jnp.array(subject_positions),jnp.array(goal_positions))/maximum_euclidian_dist\n",
    "subjects_df[\"norm_distance_to_goal\"] = list(all_euclidian_distances)\n",
    "\n",
    "\n",
    "# 4/ Data from the realized actions :\n",
    "\n",
    "# Actions performed : this encompasses the points dropped\n",
    "# But may also include temporal elements such as :\n",
    "# - the time taken to perform an actions (first point / second point)\n",
    "# - when the action was performed with regard to the gauge\n",
    "canvas_size = TASK_RESULTS[0][0][\"canvas_size\"] # Constant across all subjects + conditions\n",
    "all_actions_data = np.stack([subjdata[2][\"blanket\"][\"actions\"] for subjdata in TASK_RESULTS]).astype(float)\n",
    "\n",
    "Nsubj,Ntrials,Nactions,Npoints,Nfeatures = all_actions_data.shape\n",
    "# print(all_actions_data)\n",
    "# Normalize the point data :\n",
    "all_actions_data[...,0] = all_actions_data[...,0]/canvas_size[0]\n",
    "all_actions_data[...,1] = 1.0 - all_actions_data[...,1]/canvas_size[1]\n",
    "\n",
    "print(all_actions_data.shape)\n",
    "\n",
    "# First, let's get a mask for all actions that were NOT performed :\n",
    "mask = all_actions_data[...,-1]==1  # values are 1 if the point was recorded\n",
    "both_points_only = (mask[...,0] & mask[...,1])\n",
    "     # All points where at least one value is missing\n",
    "\n",
    "Nactions = all_actions_data[...,0,0].size\n",
    "Nmissed_actions = (~both_points_only).sum()\n",
    "print(\"A total of {}/{} actions were missed. ({:.2f} %)\".format(Nmissed_actions,Nactions,100*Nmissed_actions/Nactions))\n",
    "\n",
    "subjects_df[\"raw_points\"] = list(all_actions_data)\n",
    "\n",
    "\n",
    "# Encoded barycenters :\n",
    "barycenter_x = (all_actions_data[...,0,0]+all_actions_data[...,1,0])/2.0\n",
    "barycenter_y = (all_actions_data[...,0,1]+all_actions_data[...,1,1])/2.0\n",
    "barycenters = np.stack([barycenter_x,barycenter_y],axis=-1)\n",
    "subjects_df[\"action_barycenters\"] = list(barycenters)\n",
    "\n",
    "# Encoded euclidian distance between points :\n",
    "action_distances = np.linalg.norm(all_actions_data[...,0,:2]-all_actions_data[...,1,:2],axis=-1)\n",
    "subjects_df[\"action_distances\"] = list(action_distances)\n",
    "\n",
    "# Encoded evolution of point angles :\n",
    "angles = np.atan2(all_actions_data[...,1,1]-all_actions_data[...,0,1],all_actions_data[...,1,0]-all_actions_data[...,0,0])\n",
    "subjects_df[\"action_angles\"] = list(angles)\n",
    "\n",
    "# Encoded delays between stimuli, point1 and point2 :\n",
    "all_action_delays = all_actions_data[...,-1,2]\n",
    "unfit_actions = (all_action_delays<10)\n",
    "subjects_df[\"action_time_between_points\"] = np.where(all_action_delays>10, all_action_delays, np.nan).tolist()\n",
    "\n",
    "# Performance metric : we use the average distance to goal state across the lask k_T trials and the last k_t timesteps : (ignoring the blind trial)\n",
    "last_k_trials,last_t_timesteps = 3,5\n",
    "all_distances_to_goal = np.mean(np.stack(subjects_df[\"norm_distance_to_goal\"])[:,-last_k_trials:-1,-last_t_timesteps:],axis=(-1,-2))\n",
    "subjects_df[\"final_performance\"] = (1.0 - all_distances_to_goal).tolist()\n",
    "\n",
    "\n",
    "# And for the blind trial :\n",
    "blind_trial_distances_to_goal = np.mean(np.stack(subjects_df[\"norm_distance_to_goal\"])[:,-1,-last_t_timesteps:],axis=(-1))\n",
    "subjects_df[\"blind_trial_performance\"] = (1.0 - blind_trial_distances_to_goal).tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploit_results_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
