{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational modeling : likelihood for various RL agents\n",
    "\n",
    "Question : under very low amounts of evidence, how do human sample a complex action space ? Can we infer some form of structure in this exploration ? Can Active Inference provide some answers regarding the mechanistic processes behind it ?\n",
    "\n",
    "In notebook 103, we derived a few proposal models to explain the behaviour of our subjects.  However, this is not enough as we aim at performing model inversion based on task data ! This means that we're going to need **likelihood functions** for each of these models !\n",
    "\n",
    "Likelihood function describe the probability of these models generating the observed actions, given their hyperparameters $\\theta$ and their previous experiences $o_{1:T,1:t},s_{1:T,1:t}$ : \n",
    "$$\n",
    "\\prod_T \\prod_{t\\in T} P(u_t|o_{1:T,1:t},u_{1:T,1:t-1},\\theta)\n",
    "$$\n",
    "\n",
    "In this notebook, we modify the previous models to compute their likelihood in a jax environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed packages \n",
    "# \n",
    "# 1/ the usual suspects\n",
    "import sys,os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax import vmap\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# 2/ The Active Inference package \n",
    "import actynf\n",
    "from actynf.jaxtynf.jax_toolbox import _normalize,_jaxlog\n",
    "from actynf.jaxtynf.layer_trial import compute_step_posteriors\n",
    "from actynf.jaxtynf.layer_learn import learn_after_trial\n",
    "from actynf.jaxtynf.layer_options import get_learning_options,get_planning_options\n",
    "from actynf.jaxtynf.shape_tools import to_log_space,get_vectorized_novelty\n",
    "\n",
    "from actynf.jaxtynf.layer_process import initial_state_and_obs,process_update\n",
    "from actynf.jaxtynf.shape_tools import vectorize_weights\n",
    "\n",
    "\n",
    "# 3/ Tools for : \n",
    "# a. Getting the raw data : \n",
    "from database_handling.database_extract import get_all_subject_data_from_internal_task_id\n",
    "from utils import remove_by_indices\n",
    "# b. Preprocessing the data :\n",
    "from analysis_tools.preprocess import OPTIONS_PREPROCESS_DEFAULT,get_preprocessed_data\n",
    "\n",
    "\n",
    "\n",
    "# The environment is statically defined by its HMM matrices : \n",
    "from hmm_weights import behavioural_process\n",
    "# Weights for the active inference model : \n",
    "from hmm_weights import basic_latent_model\n",
    "\n",
    "\n",
    "# To generate synthetic data :\n",
    "from simulate.generate_observations import TrainingEnvironment,run_loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantitate some agents and generate some synthetic data for the random agent only. Our goal here is to compute,for each model, the likelihood of having performed the actions seen here for a given set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n"
     ]
    }
   ],
   "source": [
    "# The synthetic environment CONSTANTS :\n",
    "T = 11\n",
    "N_FEEDBACK_OUTCOMES = 10\n",
    "TRUE_FEEDBACK_STD = 0.15\n",
    "GRID_SIZE = (7,7)\n",
    "START_COORD = [[5,1],[5,2],[4,1]]\n",
    "END_COORD = [0,6]\n",
    "(a,b,c,d,e,u),fb_vals = behavioural_process(GRID_SIZE,START_COORD,END_COORD,N_FEEDBACK_OUTCOMES,TRUE_FEEDBACK_STD)\n",
    "rngkey = jax.random.PRNGKey(np.random.randint(0,10))\n",
    "ENVIRONMENT = TrainingEnvironment(rngkey,a,b,c,d,e,u,T)\n",
    "\n",
    "\n",
    "# In : an agent based on some hyperparameters : \n",
    "from agents import random_agent\n",
    "SEED = 100\n",
    "NTRIALS = 10\n",
    "random_agent_hyperparameters = None\n",
    "random_agent_constants = (9,)\n",
    "\n",
    "# Synthetic data (here, generated randomly) :\n",
    "params_final,training_hist = run_loop(ENVIRONMENT,random_agent(random_agent_hyperparameters,random_agent_constants),SEED,NTRIALS)\n",
    "# Parameter update (once every trial)\n",
    "def _swaplist(_list):\n",
    "    \"\"\" Put the various factors / modalities as the leading dimension for a 2D list of lists.\"\"\"\n",
    "    if _list is None :\n",
    "        return None\n",
    "    \n",
    "    for el in _list :\n",
    "        if (type(el) != list) and (type(el) != tuple):\n",
    "            # There is a single factor here ! \n",
    "            return _list\n",
    "    \n",
    "    _swapped_list = []\n",
    "    for factor in range(len(_list[0])):\n",
    "        _swapped_list.append([_el[factor] for _el in _list])\n",
    "    return _swapped_list\n",
    "        \n",
    "        \n",
    "formatted_stimuli= [jnp.array(o) for o in _swaplist(training_hist[\"stimuli\"])]\n",
    "bool_stimuli = [jnp.ones_like(stim[...,0]) for stim in formatted_stimuli]\n",
    "rewards = jnp.array(training_hist[\"rewards\"])\n",
    "SYNTHETIC_ACTIONS = jnp.array(training_hist[\"actions\"])\n",
    "tmtsp = jnp.array(training_hist[\"timestamps\"])\n",
    "SYNTHETIC_DATA = (formatted_stimuli,bool_stimuli,rewards,SYNTHETIC_ACTIONS,tmtsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the generic function that will help us compute the training likelihood. Our goal is to make it work for all kinds of agents implementing the key (parametric) methods : init_params,init_state,agent_learn and predict !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]\n",
      "\n",
      " [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]]]\n",
      "-2.1972246\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_predicted_actions(data,agent_functions):\n",
    "    \"\"\"A function that uses vmap to compute the predicted agent action at time $t$ given $o_{1:t}$ and $u_{1:t-1}$. \n",
    "    This function should be differentiable w.r.t. the hyperparameters of the agent's model because we're going to perform\n",
    "    gradient descent on it !\n",
    "\n",
    "    Args:\n",
    "        environment (_type_): _description_\n",
    "        agent_functions (_type_): _description_\n",
    "        seed (_type_): _description_\n",
    "        Ntrials (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    init_params,init_state,_,agent_learn,predict,_ = agent_functions\n",
    "    \n",
    "    \n",
    "    # Data should contain :\n",
    "    # - all observations -> stimuli,reward (from the system)\n",
    "    #       -> a list of stimuli for each modality\n",
    "    #       -> a list of observation filters for each modality\n",
    "    #       -> a Ntrials x Ntimesteps tensor array of scalar rewards (\\in [0,1])\n",
    "    # - all true actions \n",
    "    #       -> a Ntrials x (Ntimesteps-1) x Nu tensor array encoding the observed actions\n",
    "    #       -> a Ntrials x (Ntimesteps-1) filter tensor indicating which actions were NOT observed\n",
    "    \n",
    "    initial_parameters = init_params()  \n",
    "        # The initial parameters of the tested model are initialized once per training\n",
    "    \n",
    "    \n",
    "    def _scan_trial(_carry,_data_trial):\n",
    "        \n",
    "        _agent_params = _carry\n",
    "        _initial_state = init_state(_agent_params)\n",
    "        \n",
    "        _observations_trial,_observations_filter_trial,_rewards_trial,_actions_trial,_timestamps_trial = _data_trial\n",
    "        \n",
    "        # The same actions, with an extra one at the end for scan to work better !\n",
    "        _expanded_actions_trial = jnp.concatenate([_actions_trial,jnp.zeros((1,_actions_trial.shape[-1]))])\n",
    "        _expanded_data_trial = (_observations_trial,_observations_filter_trial,_rewards_trial,_expanded_actions_trial,_timestamps_trial)\n",
    "        \n",
    "        def __scan_timestep(__carry,__data_timestep):\n",
    "            # __obs_vect,__obs_bool,__reward,__true_action_vect,__t = __data_timestep\n",
    "            __agent_state = __carry\n",
    "                    \n",
    "            __new_state,__predicted_action,__other_data = predict(__data_timestep,__agent_state,_agent_params)        \n",
    "            \n",
    "            return __new_state,(__predicted_action,__new_state,__other_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        _,(_predicted_actions,_trial_states,_trial_other_data) = jax.lax.scan(__scan_timestep, (_initial_state),_expanded_data_trial)\n",
    "          \n",
    "        \n",
    "        _new_params = agent_learn((_rewards_trial,_observations_trial,_trial_states,_actions_trial),_agent_params)\n",
    "        \n",
    "        return _new_params,(_predicted_actions[:-1,...],(_trial_states,_trial_other_data))\n",
    "\n",
    "    final_parameters,(predicted_actions,(model_states,other_data)) = jax.lax.scan(_scan_trial,initial_parameters,data)\n",
    "\n",
    "    return final_parameters,predicted_actions,(model_states,other_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example :  compute the likelihood of each action given the random model : \n",
    "final_parameters,predicted_actions,model_states = compute_predicted_actions(SYNTHETIC_DATA,random_agent(random_agent_hyperparameters,random_agent_constants))\n",
    "print(predicted_actions)\n",
    "# print(model_states)\n",
    "\n",
    "# Here's the average log-likelihood of what was observed given this model :\n",
    "avg_ll = jnp.mean((SYNTHETIC_ACTIONS * _jaxlog(predicted_actions)).sum(axis=-1))\n",
    "print(avg_ll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 11, 9)\n",
      "[9.7656250e-03 1.0768622e-03 4.0283222e-03 2.5001526e-01 2.4414156e-04\n",
      " 4.8834115e-04 5.0000000e-01 5.7220791e-06 2.3437572e-01]\n",
      "-4.498164\n"
     ]
    }
   ],
   "source": [
    "from agents import choice_kernel_agent\n",
    "\n",
    "ck_agent_hyperparameters = (0.5,10.0)   # [0,1] x [0, +oo]\n",
    "ck_agent_constants = (9,)              # Nactions\n",
    "final_parameters,predicted_actions,state_history = compute_predicted_actions(SYNTHETIC_DATA,choice_kernel_agent(ck_agent_hyperparameters,ck_agent_constants))\n",
    "# print(predicted_actions)\n",
    "\n",
    "\n",
    "# We can have an idea of what happened during training by looking at the inner states of the model\n",
    "inner_states,_ = state_history\n",
    "# Of course, these will vary from one model to the next :\n",
    "ck_table,previous_action = inner_states\n",
    "print(ck_table.shape)\n",
    "print(ck_table[-1,-1,:])\n",
    "\n",
    "# Here's the average log-likelihood of what was observed given this model :\n",
    "avg_ll = jnp.mean((SYNTHETIC_ACTIONS * _jaxlog(predicted_actions)).sum(axis=-1))\n",
    "print(avg_ll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very \"strict\" choice kernel agent is very unlikely for this data (which makes sense, as it was randomly generated ! )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "   0.11111111 0.11111111 0.11111111]\n",
      "  [0.09059832 0.09059832 0.2752134  0.09059832 0.09059832 0.09059832\n",
      "   0.09059832 0.09059832 0.09059832]\n",
      "  [0.10263875 0.10263875 0.17889002 0.10263875 0.10263875 0.10263875\n",
      "   0.10263875 0.10263875 0.10263875]\n",
      "  [0.10263875 0.10263875 0.17889002 0.10263875 0.10263875 0.10263875\n",
      "   0.10263875 0.10263875 0.10263875]\n",
      "  [0.10729392 0.10729392 0.14164865 0.10729392 0.10729392 0.10729392\n",
      "   0.10729392 0.10729392 0.10729392]\n",
      "  [0.11561515 0.11561515 0.15263428 0.11561515 0.11561515 0.11561515\n",
      "   0.11561515 0.11561515 0.03805969]\n",
      "  [0.05925152 0.05925152 0.07822344 0.05925152 0.54676217 0.05925152\n",
      "   0.05925152 0.05925152 0.01950518]\n",
      "  [0.05925152 0.05925152 0.07822344 0.05925152 0.54676217 0.05925152\n",
      "   0.05925152 0.05925152 0.01950518]\n",
      "  [0.10647024 0.10647024 0.14056127 0.10647024 0.18556795 0.10647024\n",
      "   0.10647024 0.10647024 0.03504926]]\n",
      "\n",
      " [[0.06831005 0.06831005 0.09018245 0.06831005 0.47747004 0.06831005\n",
      "   0.06831005 0.06831005 0.02248719]\n",
      "  [0.07298572 0.07298572 0.09635522 0.07298572 0.51015174 0.07298572\n",
      "   0.07298572 0.004538   0.02402639]\n",
      "  [0.06353627 0.06353627 0.08388014 0.06353627 0.44410253 0.19300616\n",
      "   0.06353627 0.00395046 0.0209157 ]\n",
      "  [0.06636477 0.06636477 0.0876143  0.02184682 0.463873   0.20159838\n",
      "   0.06636477 0.00412633 0.02184682]\n",
      "  [0.09326358 0.09326358 0.12312588 0.03070171 0.24657142 0.28330973\n",
      "   0.09326358 0.0057988  0.03070171]\n",
      "  [0.08768313 0.08768313 0.1755939  0.02886466 0.23181775 0.2663578\n",
      "   0.08768313 0.00545183 0.02886466]\n",
      "  [0.10675809 0.10675809 0.21379335 0.035144   0.2822484  0.10675809\n",
      "   0.10675809 0.00663785 0.035144  ]\n",
      "  [0.09891313 0.09891313 0.19808307 0.0325615  0.2615078  0.09891313\n",
      "   0.1723966  0.00615007 0.0325615 ]\n",
      "  [0.09891313 0.09891313 0.19808307 0.0325615  0.2615078  0.09891313\n",
      "   0.1723966  0.00615007 0.0325615 ]\n",
      "  [0.25006822 0.08232071 0.16485514 0.02709939 0.21764052 0.08232071\n",
      "   0.14347748 0.00511841 0.02709939]]\n",
      "\n",
      " [[0.25006822 0.08232071 0.16485514 0.02709939 0.21764052 0.08232071\n",
      "   0.14347748 0.00511841 0.02709939]\n",
      "  [0.24513312 0.08069611 0.16160172 0.04629967 0.21334538 0.08069611\n",
      "   0.14064595 0.0050174  0.02656459]\n",
      "  [0.24794054 0.08162029 0.16345249 0.04682992 0.21578875 0.08162029\n",
      "   0.14225672 0.00507487 0.01541608]\n",
      "  [0.20921974 0.06887366 0.13792613 0.03951649 0.18208909 0.06887366\n",
      "   0.27621037 0.00428232 0.01300855]\n",
      "  [0.20660557 0.06801309 0.13620277 0.05151755 0.17981392 0.06801309\n",
      "   0.27275917 0.00422882 0.01284601]\n",
      "  [0.21277398 0.04018781 0.14026923 0.05305566 0.18518244 0.07004369\n",
      "   0.28090265 0.00435507 0.01322954]\n",
      "  [0.18619788 0.03516823 0.12274919 0.04642885 0.1620526  0.18619788\n",
      "   0.24581708 0.00381111 0.01157713]\n",
      "  [0.18343507 0.0346464  0.12092783 0.04573993 0.15964806 0.18343507\n",
      "   0.24216963 0.00375456 0.02624344]\n",
      "  [0.11417354 0.03758513 0.13118501 0.04961962 0.17318949 0.19899413\n",
      "   0.2627106  0.00407303 0.02846942]\n",
      "  [0.11281587 0.04902958 0.12962504 0.04902958 0.17113005 0.19662784\n",
      "   0.25958663 0.00402459 0.02813089]]\n",
      "\n",
      " [[0.1384607  0.06017477 0.15909088 0.06017477 0.21003059 0.24132442\n",
      "   0.09127893 0.00493945 0.03452548]\n",
      "  [0.08775322 0.40436038 0.10082813 0.0381374  0.1331126  0.1529459\n",
      "   0.05785049 0.00313051 0.02188146]\n",
      "  [0.08527358 0.42119128 0.09797904 0.03705974 0.12935121 0.14862409\n",
      "   0.05621582 0.00304205 0.02126316]\n",
      "  [0.09830011 0.48553318 0.11294649 0.04272105 0.14911114 0.01856649\n",
      "   0.06480344 0.00350676 0.02451135]\n",
      "  [0.09785368 0.4833281  0.11243353 0.04252703 0.14843394 0.01848217\n",
      "   0.06450914 0.00803232 0.02440003]\n",
      "  [0.100878   0.4982661  0.11590847 0.04384139 0.15302151 0.01905339\n",
      "   0.03559643 0.00828057 0.02515415]\n",
      "  [0.09952331 0.4915749  0.11435194 0.04325265 0.1509666  0.01879752\n",
      "   0.03511841 0.02159829 0.02481636]\n",
      "  [0.10297774 0.5086373  0.08361138 0.04475393 0.1562066  0.01944998\n",
      "   0.03633735 0.02234796 0.02567773]\n",
      "  [0.1095668  0.54118264 0.08896128 0.04761753 0.1022162  0.02069449\n",
      "   0.03866241 0.0237779  0.02732073]\n",
      "  [0.22065726 0.47366467 0.07786246 0.04167676 0.0894637  0.01811265\n",
      "   0.03383889 0.02081137 0.02391219]]\n",
      "\n",
      " [[0.2179632  0.4678816  0.07691182 0.04116792 0.08837142 0.01789151\n",
      "   0.03342574 0.02055728 0.03582946]\n",
      "  [0.19469875 0.41794193 0.1754382  0.03677383 0.07893904 0.01598184\n",
      "   0.02985802 0.01836308 0.03200518]\n",
      "  [0.19248617 0.41319236 0.1734445  0.03635593 0.07804196 0.01580022\n",
      "   0.02951871 0.0295187  0.03164147]\n",
      "  [0.06392796 0.47897363 0.20105729 0.04214389 0.09046643 0.01831566\n",
      "   0.03421816 0.03421815 0.03667887]\n",
      "  [0.06352904 0.47598478 0.19980267 0.04812102 0.08990191 0.01820137\n",
      "   0.03400464 0.03400463 0.03644999]\n",
      "  [0.05650697 0.4233727  0.17771785 0.04280205 0.19049795 0.01618951\n",
      "   0.030246   0.03024599 0.03242106]\n",
      "  [0.01806089 0.44062454 0.1849596  0.04454618 0.19826047 0.01684921\n",
      "   0.03147848 0.03147848 0.03374217]\n",
      "  [0.01738815 0.42421216 0.17807023 0.04288692 0.19087566 0.01622161\n",
      "   0.03030597 0.03030596 0.06973331]\n",
      "  [0.01753074 0.42769083 0.17953047 0.04323861 0.19244091 0.01635464\n",
      "   0.03055449 0.02235414 0.07030515]\n",
      "  [0.02087622 0.5093091  0.21379112 0.05149003 0.03833063 0.01947567\n",
      "   0.03638535 0.02662008 0.08372182]]\n",
      "\n",
      " [[0.01333332 0.32528788 0.13654512 0.03288589 0.02448118 0.01243881\n",
      "   0.02323876 0.01700184 0.41478732]\n",
      "  [0.00873581 0.21312414 0.43427616 0.02154638 0.01603973 0.00814974\n",
      "   0.01522572 0.01113937 0.27176294]\n",
      "  [0.0119124  0.29062238 0.5921918  0.02938129 0.02187225 0.01111322\n",
      "   0.02076224 0.01518997 0.00695448]\n",
      "  [0.00695594 0.16970143 0.76187086 0.01715645 0.01277174 0.00648928\n",
      "   0.01212357 0.0088698  0.00406089]\n",
      "  [0.00685393 0.16721277 0.7506981  0.01690485 0.02724938 0.00639411\n",
      "   0.01194578 0.00873972 0.00400134]\n",
      "  [0.01148432 0.16643317 0.7471981  0.01682603 0.02712233 0.0063643\n",
      "   0.01189009 0.00869897 0.00398268]\n",
      "  [0.01231652 0.10602969 0.80134296 0.01804531 0.02908772 0.00682549\n",
      "   0.01275169 0.00932933 0.00427128]\n",
      "  [0.01239963 0.10674514 0.8067501  0.01816707 0.02928399 0.00687154\n",
      "   0.01283773 0.00264462 0.0043001 ]\n",
      "  [0.0119114  0.10254212 0.7749849  0.05682611 0.02813096 0.00660098\n",
      "   0.01233225 0.00254049 0.00413079]\n",
      "  [0.02693148 0.23184608 0.49124455 0.12848292 0.06360365 0.01492471\n",
      "   0.02788303 0.00574401 0.00933964]]\n",
      "\n",
      " [[0.02683715 0.231034   0.48952386 0.12803288 0.06338086 0.01487243\n",
      "   0.02778537 0.00922649 0.00930693]\n",
      "  [0.09803631 0.21413095 0.45370904 0.11866567 0.05874375 0.01378433\n",
      "   0.02575252 0.00855146 0.00862601]\n",
      "  [0.09781346 0.21364419 0.45267767 0.11839592 0.05861022 0.01375299\n",
      "   0.02569398 0.00853202 0.01087951]\n",
      "  [0.09804061 0.21414034 0.45372894 0.11867087 0.05874633 0.01378493\n",
      "   0.02575365 0.00622956 0.01090478]\n",
      "  [0.10300303 0.22497925 0.47669485 0.0740616  0.06171983 0.01448267\n",
      "   0.02705719 0.00654487 0.01145673]\n",
      "  [0.10860497 0.18282884 0.50262046 0.07808953 0.06507654 0.01527033\n",
      "   0.02852873 0.00690082 0.01207982]\n",
      "  [0.10368519 0.17454675 0.4798519  0.0745521  0.06212859 0.01457858\n",
      "   0.02723639 0.05188787 0.01153261]\n",
      "  [0.10470919 0.17627057 0.48459092 0.07528837 0.06274217 0.00484657\n",
      "   0.02750537 0.05240031 0.01164651]\n",
      "  [0.10184421 0.17144758 0.47133186 0.07322838 0.06102546 0.00471396\n",
      "   0.02675279 0.05096657 0.03868927]\n",
      "  [0.09508359 0.16006656 0.44004396 0.06836734 0.05697447 0.07078291\n",
      "   0.02497689 0.04758332 0.036121  ]]\n",
      "\n",
      " [[0.09648184 0.16242042 0.44651502 0.06937271 0.05781231 0.07182381\n",
      "   0.01063865 0.04828305 0.03665218]\n",
      "  [0.09944609 0.16741051 0.46023345 0.07150408 0.02886519 0.07403047\n",
      "   0.01096551 0.04976647 0.03777825]\n",
      "  [0.14633408 0.24634315 0.2057381  0.10521764 0.04247488 0.1089352\n",
      "   0.01613565 0.07323093 0.05559038]\n",
      "  [0.1460296  0.2458306  0.20531002 0.10499872 0.04238651 0.10870854\n",
      "   0.0181828  0.07307856 0.05547471]\n",
      "  [0.14380161 0.24207993 0.20217758 0.10339674 0.04173981 0.10704996\n",
      "   0.03316253 0.07196359 0.05462833]\n",
      "  [0.12744622 0.3282826  0.17918274 0.09163681 0.03699249 0.09487453\n",
      "   0.02939076 0.06377875 0.04841512]\n",
      "  [0.18285859 0.03622643 0.25708964 0.1314796  0.05307646 0.13612504\n",
      "   0.04216958 0.09150912 0.06946555]\n",
      "  [0.10072885 0.01995557 0.1416195  0.0724264  0.02923752 0.52412885\n",
      "   0.02322939 0.05040839 0.03826555]\n",
      "  [0.10177816 0.00974618 0.14309478 0.07318088 0.02954209 0.5295888\n",
      "   0.02347137 0.05093351 0.03866417]\n",
      "  [0.09854677 0.00943674 0.13855161 0.07085744 0.02860415 0.5127747\n",
      "   0.02272617 0.0493164  0.06918599]]\n",
      "\n",
      " [[0.09001825 0.00862006 0.12656094 0.06472522 0.02612866 0.46839768\n",
      "   0.02075938 0.04504842 0.1497414 ]\n",
      "  [0.07764385 0.14490046 0.10916319 0.05582774 0.02253687 0.4040092\n",
      "   0.01790568 0.03885582 0.12915713]\n",
      "  [0.07908799 0.14759554 0.11119358 0.05686611 0.02295605 0.41152358\n",
      "   0.01823872 0.02097897 0.13155939]\n",
      "  [0.0786443  0.14676751 0.11056977 0.05654709 0.02282726 0.40921488\n",
      "   0.0181364  0.02647147 0.13082133]\n",
      "  [0.08516391 0.07603455 0.11973601 0.06123484 0.02471964 0.4431388\n",
      "   0.01963991 0.02866596 0.14166643]\n",
      "  [0.08817566 0.07872345 0.12397037 0.02803623 0.02559383 0.45881003\n",
      "   0.02033446 0.0296797  0.14667633]\n",
      "  [0.07289875 0.23833951 0.10249184 0.0231788  0.02115957 0.37931874\n",
      "   0.0168114  0.02453754 0.12126388]\n",
      "  [0.0922747  0.03589592 0.1297334  0.02933955 0.02678362 0.4801388\n",
      "   0.02127975 0.03105943 0.1534949 ]\n",
      "  [0.09274821 0.03608012 0.12526761 0.02949011 0.02692106 0.48260266\n",
      "   0.02138895 0.03121881 0.15428255]\n",
      "  [0.09221926 0.03587436 0.1245532  0.02932193 0.0324705  0.47985038\n",
      "   0.02126697 0.03104077 0.15340269]]\n",
      "\n",
      " [[0.07389165 0.02874471 0.09979959 0.0234945  0.02601733 0.58322453\n",
      "   0.01704038 0.02487175 0.12291551]\n",
      "  [0.0737902  0.03007834 0.09966256 0.02346225 0.02598161 0.58242375\n",
      "   0.01701698 0.0248376  0.12274675]\n",
      "  [0.01685129 0.03192741 0.10578933 0.02490459 0.02757883 0.6182283\n",
      "   0.0180631  0.02636449 0.13029262]\n",
      "  [0.01567667 0.02970192 0.16812003 0.02316863 0.02565646 0.57513493\n",
      "   0.01680402 0.02452677 0.12121062]\n",
      "  [0.10619299 0.0269706  0.15266006 0.02103809 0.02329714 0.5222467\n",
      "   0.01525876 0.02227134 0.11006434]\n",
      "  [0.08959777 0.02275579 0.12880324 0.01775038 0.0196564  0.44063306\n",
      "   0.01287421 0.0187909  0.2491383 ]\n",
      "  [0.11907641 0.03024269 0.17118089 0.02359044 0.02612357 0.5856061\n",
      "   0.01710996 0.02497331 0.00209661]\n",
      "  [0.11765166 0.02988084 0.16913271 0.02330818 0.025811   0.57859933\n",
      "   0.01690524 0.02467451 0.01403663]\n",
      "  [0.11506255 0.02922326 0.16541068 0.02279525 0.02524299 0.56586635\n",
      "   0.01653322 0.0241315  0.03573426]\n",
      "  [0.11594801 0.02944815 0.1666836  0.01527522 0.02543725 0.57022095\n",
      "   0.01666045 0.02431721 0.03600925]]]\n",
      "(10, 11, 9)\n",
      "[ 0.1323785  -0.00467258  0.16867405 -0.0703125  -0.01931424  0.2916667\n",
      " -0.08637153 -0.02381727  0.0154419 ]\n",
      "-2.818392\n"
     ]
    }
   ],
   "source": [
    "from agents import rescorla_wagner_agent\n",
    "\n",
    "rw_agent_hyperparameters = (0.5,10.0)\n",
    "rw_agent_constants = (9,)\n",
    "final_parameters,predicted_actions,state_history = compute_predicted_actions(SYNTHETIC_DATA,rescorla_wagner_agent(rw_agent_hyperparameters,rw_agent_constants))\n",
    "print(predicted_actions)\n",
    "\n",
    "\n",
    "# We can have an idea of what happened during training by looking at the inner states of the model\n",
    "inner_states,_ = state_history\n",
    "# Of course, these will vary from one model to the next :\n",
    "q_table,previous_action = inner_states\n",
    "print(q_table.shape)\n",
    "print(q_table[-1,-1,:])\n",
    "\n",
    "# Here's the average log-likelihood of what was observed given this model :\n",
    "avg_ll = jnp.mean((SYNTHETIC_ACTIONS * _jaxlog(predicted_actions)).sum(axis=-1))\n",
    "print(avg_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 11, 9)\n",
      "[ 0.03156808  0.04391102  0.11929256 -0.01553993  0.02575896  0.07111171\n",
      " -0.0330379  -0.06475359 -0.00941168]\n",
      "[9.7656250e-03 1.0768622e-03 4.0283222e-03 2.5001526e-01 2.4414156e-04\n",
      " 4.8834115e-04 5.0000000e-01 5.7220791e-06 2.3437572e-01]\n",
      "-2.2950704\n"
     ]
    }
   ],
   "source": [
    "from agents import rw_ck_agent\n",
    "\n",
    "rw_ck_agent_hyperparameters = (0.1,10.0,0.5,0.1)\n",
    "rw_ck_agent_constants = (9,)\n",
    "final_parameters,predicted_actions,state_history = compute_predicted_actions(SYNTHETIC_DATA,rw_ck_agent(rw_ck_agent_hyperparameters,rw_ck_agent_constants))\n",
    "\n",
    "# We can have an idea of what happened during training by looking at the inner states of the model\n",
    "inner_states,_ = state_history\n",
    "# Of course, these will vary from one model to the next :\n",
    "q_table,ck_table,previous_action = inner_states\n",
    "print(q_table.shape)\n",
    "print(q_table[-1,-1,:])\n",
    "print(ck_table[-1,-1,:])\n",
    "\n",
    "# Here's the average log-likelihood of what was observed given this model :\n",
    "avg_ll = jnp.mean((SYNTHETIC_ACTIONS * _jaxlog(predicted_actions)).sum(axis=-1))\n",
    "print(avg_ll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 11, 9, 10)\n",
      "[[ 0.          0.09333334  0.1977778  -0.07777778 -0.15555556  0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.22        0.11111112 -0.03644444  0.04666667 -0.17777778 -0.15555556\n",
      "   0.05555555  0.          0.          0.        ]\n",
      " [ 0.1         0.19911112 -0.04444444  0.05555555  0.          0.\n",
      "   0.05555555  0.          0.          0.        ]\n",
      " [ 0.          0.          0.00124444 -0.06222223  0.11111112  0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.         -0.04444444 -0.1777778  -0.07777778 -0.03333333  0.07111112\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.2         0.14506668  0.         -0.23333333  0.16666667  0.\n",
      "  -0.23333333  0.          0.          0.        ]\n",
      " [ 0.1         0.         -0.06533334 -0.07777778 -0.15555556  0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.11111112 -0.15555556  0.01617778 -0.07777777 -0.3888889\n",
      "   0.         -0.03111111  0.          0.        ]\n",
      " [ 0.01111111  0.05555556  0.04791111  0.12641779  0.          0.16666666\n",
      "  -0.3888889   0.         -0.54444444  0.        ]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-2.189752\n"
     ]
    }
   ],
   "source": [
    "from agents import q_learning_agent\n",
    "\n",
    "ql_ck_agent_hyperparameters = (0.5,0.7,1.0,0.0,0.0)\n",
    "ql_ck_agent_constants = (9,N_FEEDBACK_OUTCOMES)\n",
    "final_parameters,predicted_actions,state_history = compute_predicted_actions(SYNTHETIC_DATA,q_learning_agent(ql_ck_agent_hyperparameters,ql_ck_agent_constants))\n",
    "\n",
    "# We can have an idea of what happened during training by looking at the inner states of the model\n",
    "inner_states,_ = state_history\n",
    "# Of course, these will vary from one model to the next :\n",
    "q_table,ck_table,previous_action,previous_stim = inner_states\n",
    "print(q_table.shape)\n",
    "print(q_table[-1,-1,:])\n",
    "print(ck_table[-1,-1,:])\n",
    "\n",
    "# Here's the average log-likelihood of what was observed given this model :\n",
    "avg_ll = jnp.mean((SYNTHETIC_ACTIONS * _jaxlog(predicted_actions)).sum(axis=-1))\n",
    "print(avg_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(10, 5)\n",
      "(11, 5)\n",
      "[[[-2.1938996]\n",
      "  [-2.010765 ]\n",
      "  [-2.4078493]\n",
      "  [-2.3040066]\n",
      "  [-2.2940648]\n",
      "  [-2.293491 ]\n",
      "  [-2.145039 ]\n",
      "  [-2.5764241]\n",
      "  [-2.3298213]\n",
      "  [-2.320813 ]\n",
      "  [-2.3812044]]\n",
      "\n",
      " [[-2.480289 ]\n",
      "  [-2.2362301]\n",
      "  [-2.2522225]\n",
      "  [-1.9728615]\n",
      "  [-1.8863776]\n",
      "  [-1.883971 ]\n",
      "  [-1.8614244]\n",
      "  [-1.9330776]\n",
      "  [-1.9916298]\n",
      "  [-2.4168468]\n",
      "  [-2.305314 ]]\n",
      "\n",
      " [[-2.3424745]\n",
      "  [-2.1649468]\n",
      "  [-1.8932033]\n",
      "  [-2.1510468]\n",
      "  [-2.167744 ]\n",
      "  [-2.0357294]\n",
      "  [-2.278522 ]\n",
      "  [-2.2814672]\n",
      "  [-2.2566822]\n",
      "  [-2.2480063]\n",
      "  [-2.1387062]]\n",
      "\n",
      " [[-2.1043572]\n",
      "  [-2.5953283]\n",
      "  [-2.4123223]\n",
      "  [-2.3893862]\n",
      "  [-2.1949158]\n",
      "  [-1.9402412]\n",
      "  [-1.9331737]\n",
      "  [-1.8618709]\n",
      "  [-1.96731  ]\n",
      "  [-2.292948 ]\n",
      "  [-2.2384915]]\n",
      "\n",
      " [[-2.1938996]\n",
      "  [-2.25671  ]\n",
      "  [-2.2333448]\n",
      "  [-2.1144917]\n",
      "  [-2.0265908]\n",
      "  [-2.3597898]\n",
      "  [-2.241665 ]\n",
      "  [-2.2184696]\n",
      "  [-2.0605414]\n",
      "  [-1.9952385]\n",
      "  [-2.6123257]]\n",
      "\n",
      " [[-2.3424745]\n",
      "  [-2.638266 ]\n",
      "  [-2.2877254]\n",
      "  [-2.5894203]\n",
      "  [-2.266792 ]\n",
      "  [-2.2753854]\n",
      "  [-2.1701996]\n",
      "  [-2.455135 ]\n",
      "  [-2.418582 ]\n",
      "  [-2.2483063]\n",
      "  [-2.327612 ]]\n",
      "\n",
      " [[-2.1938996]\n",
      "  [-2.210709 ]\n",
      "  [-2.018192 ]\n",
      "  [-1.7789912]\n",
      "  [-1.6114972]\n",
      "  [-1.9284284]\n",
      "  [-2.3468466]\n",
      "  [-1.9840176]\n",
      "  [-1.8558203]\n",
      "  [-2.5111601]\n",
      "  [-2.1126392]]\n",
      "\n",
      " [[-2.4443073]\n",
      "  [-2.2217417]\n",
      "  [-2.0882294]\n",
      "  [-1.8763895]\n",
      "  [-1.7929957]\n",
      "  [-2.2504401]\n",
      "  [-2.1465151]\n",
      "  [-2.48495  ]\n",
      "  [-2.2255979]\n",
      "  [-2.1458359]\n",
      "  [-2.5061722]]\n",
      "\n",
      " [[-2.1043572]\n",
      "  [-2.4503179]\n",
      "  [-2.166223 ]\n",
      "  [-2.1468482]\n",
      "  [-2.1477985]\n",
      "  [-2.0341995]\n",
      "  [-2.497509 ]\n",
      "  [-2.2468185]\n",
      "  [-2.186173 ]\n",
      "  [-2.206966 ]\n",
      "  [-2.4644287]]\n",
      "\n",
      " [[-2.3424745]\n",
      "  [-2.040553 ]\n",
      "  [-2.034291 ]\n",
      "  [-2.0071175]\n",
      "  [-2.4858983]\n",
      "  [-2.5032668]\n",
      "  [-2.3489075]\n",
      "  [-2.0117598]\n",
      "  [-2.1336951]\n",
      "  [-1.9817966]\n",
      "  [-1.7999613]]]\n",
      "(10, 11, 1)\n",
      "-3.563119\n"
     ]
    }
   ],
   "source": [
    "from agents import active_inference_basic_1D\n",
    "\n",
    "\n",
    "# We get a model weights by defining a \"parameters\" object :\n",
    "aif_1d_constants = {\n",
    "    # General environment : \n",
    "    \"N_feedback_ticks\":N_FEEDBACK_OUTCOMES,\n",
    "    # Latent state space structure\n",
    "    \"Ns_latent\":5,      # For 1D\n",
    "    # Action discretization:\n",
    "    \"N_actions_distance\" :3,\n",
    "    \"N_actions_position\" :9,\n",
    "    \"N_actions_angle\" :9,\n",
    "    \n",
    "    \"Th\" : 3\n",
    "}\n",
    "\n",
    "aif_1d_params = {    \n",
    "    # ----------------------------------------------------------------------------------------------------\n",
    "    # Model parameters : these should interact with the model components in a differentiable manner\n",
    "    \"transition_concentration\": 1.0,\n",
    "    \"transition_stickiness\": 1.0,\n",
    "    \"transition_learning_rate\" : 1.0,\n",
    "    \"state_interpolation_temperature\" : 1.0,\n",
    "    \n",
    "    \"initial_state_concentration\": 1.0,\n",
    "    \n",
    "    \"feedback_expected_std\" : 0.15,\n",
    "    \"emission_concentration\" : 1.0,\n",
    "    \"emission_stickiness\" : 100.0,\n",
    "    \n",
    "    \"reward_seeking\" : 10.0,\n",
    "    \n",
    "    \"action_selection_temperature\" : 10.0,\n",
    "}\n",
    "final_parameters,predicted_actions,state_history = compute_predicted_actions(SYNTHETIC_DATA,active_inference_basic_1D(aif_1d_params,aif_1d_constants))\n",
    "# print(predicted_actions)\n",
    "\n",
    "\n",
    "# We can have an idea of what happened during training by looking at the inner states of the model\n",
    "inner_states,(infered_states,free_energies) = state_history\n",
    "# Of course, these will vary from one model to the next :\n",
    "priors,posteriors,weights = inner_states\n",
    "\n",
    "# a_norm,b_norm,c,e,a_novel,b_novel = weights\n",
    "\n",
    "print(free_energies)\n",
    "print(free_energies.shape)\n",
    "\n",
    "# print(q_table.shape)\n",
    "# print(q_table[-1,-1,:])\n",
    "# print(ck_table[-1,-1,:])\n",
    "\n",
    "# Here's the average log-likelihood of what was observed given this model :\n",
    "avg_ll = jnp.mean((SYNTHETIC_ACTIONS * _jaxlog(predicted_actions)).sum(axis=-1))\n",
    "print(avg_ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of the study, we'll stock all of these predictive models in the [proposal models](./proposal_models.py) file to manipulate and change them more easily. Next up is inversion !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploit_results_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
